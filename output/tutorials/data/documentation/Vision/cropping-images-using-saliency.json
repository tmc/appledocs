{
  "abstract": [
    {
      "text": "Isolate regions in an image that are most likely to draw people’s attention.",
      "type": "text"
    }
  ],
  "hierarchy": {
    "paths": [
      [
        "doc://com.apple.documentation/documentation/technologies",
        "doc://com.apple.vision/documentation/Vision",
        "doc://com.apple.vision/documentation/Vision/original-objective-c-and-swift-api"
      ]
    ]
  },
  "identifier": {
    "interfaceLanguage": "swift",
    "url": "doc://com.apple.vision/documentation/Vision/cropping-images-using-saliency"
  },
  "kind": "article",
  "legalNotices": {
    "copyright": "Copyright &copy; 2025 Apple Inc. All rights reserved.",
    "privacyPolicy": "https://www.apple.com/privacy/privacy-policy",
    "termsOfUse": "https://www.apple.com/legal/internet-services/terms/site.html"
  },
  "metadata": {
    "modules": [
      {
        "name": "Vision"
      }
    ],
    "role": "article",
    "roleHeading": "Article",
    "title": "Cropping Images Using Saliency"
  },
  "primaryContentSections": [
    {
      "content": [
        {
          "anchor": "overview",
          "level": 2,
          "text": "Overview",
          "type": "heading"
        },
        {
          "inlineContent": [
            {
              "inlineContent": [
                {
                  "text": "Saliency",
                  "type": "text"
                }
              ],
              "type": "emphasis"
            },
            {
              "text": " refers to what’s noticeable or important in an image. The Vision framework supports two types of saliency: ",
              "type": "text"
            },
            {
              "inlineContent": [
                {
                  "text": "object-based",
                  "type": "text"
                }
              ],
              "type": "emphasis"
            },
            {
              "text": " and ",
              "type": "text"
            },
            {
              "inlineContent": [
                {
                  "text": "attention-based",
                  "type": "text"
                }
              ],
              "type": "emphasis"
            },
            {
              "text": ".",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "Attention-based saliency highlights what people are likely to look at. Object-based saliency highlights foreground objects and provides a coarse segmentation of the main subjects in an image. The range of saliency is [0,1], where higher values indicate higher potential for interest or inclusion in the foreground.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "Saliency has many applications:",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "items": [
            {
              "content": [
                {
                  "inlineContent": [
                    {
                      "text": "Automate image cropping to fit key elements.",
                      "type": "text"
                    }
                  ],
                  "type": "paragraph"
                }
              ]
            },
            {
              "content": [
                {
                  "inlineContent": [
                    {
                      "text": "Generate thumbnails from an image set.",
                      "type": "text"
                    }
                  ],
                  "type": "paragraph"
                }
              ]
            },
            {
              "content": [
                {
                  "inlineContent": [
                    {
                      "text": "Guide the camera to focus on a key area for blur estimation or white balance.",
                      "type": "text"
                    }
                  ],
                  "type": "paragraph"
                }
              ]
            },
            {
              "content": [
                {
                  "inlineContent": [
                    {
                      "text": "Guide postprocessing by determining candidates for sharpening or lighting enhancement.",
                      "type": "text"
                    }
                  ],
                  "type": "paragraph"
                }
              ]
            },
            {
              "content": [
                {
                  "inlineContent": [
                    {
                      "text": "Pan the camera to relevant shots in a photo or video album.",
                      "type": "text"
                    }
                  ],
                  "type": "paragraph"
                }
              ]
            }
          ],
          "type": "unorderedList"
        },
        {
          "inlineContent": [
            {
              "text": "This article describes one application of saliency: cropping an input image to fit a given aspect ratio while keeping the most interesting element in the image. This technique works by focusing on the region most likely to draw attention at a glance.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "anchor": "Specify-the-Type-of-Saliency",
          "level": 3,
          "text": "Specify the Type of Saliency",
          "type": "heading"
        },
        {
          "inlineContent": [
            {
              "text": "Both models of saliency are based on deep-learning neural networks. The model used for object-based saliency is trained on foreground objects that have been segmented from the background. The model used for attention-based saliency is trained using eye-tracking data from human subjects looking at images.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "The class of the request to generate saliency differs depending on the type of saliency you want Vision to compute:",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "tabs": [
            {
              "content": [
                {
                  "code": [
                    "switch SaliencyType(rawValue: UserDefaults.standard.saliencyType)! {",
                    "    case .attentionBased:",
                    "        request = VNGenerateAttentionBasedSaliencyImageRequest()",
                    "    case .objectnessBased:",
                    "        request = VNGenerateObjectnessBasedSaliencyImageRequest()",
                    "}"
                  ],
                  "syntax": "swift",
                  "type": "codeListing"
                }
              ],
              "title": "Swift"
            },
            {
              "content": [
                {
                  "code": [
                    "switch (saliencyType) ",
                    "{",
                    "    case kAttentionBased:",
                    "        request = [[VNGenerateAttentionBasedSaliencyImageRequest alloc] init];",
                    "        break;",
                    "    case kObjectnessBased:",
                    "        request = [[VNGenerateObjectnessBasedSaliencyImageRequest alloc] init];",
                    "        break;",
                    "}"
                  ],
                  "syntax": "objc",
                  "type": "codeListing"
                }
              ],
              "title": "Objective-C"
            }
          ],
          "type": "tabNavigator"
        },
        {
          "inlineContent": [
            {
              "text": "Object-based saliency and attention-based saliency have different use cases. If you’re deciding what to keep in an image thumbnail based on what’s most interesting, use attention-based saliency.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "anchor": "Parse-the-Output-Heatmap",
          "level": 3,
          "text": "Parse the Output Heatmap",
          "type": "heading"
        },
        {
          "inlineContent": [
            {
              "text": "Both types of saliency requests return their results as heatmaps, which are 68 x 68 pixel buffers of floating-point saliency values. Think of each entry in the heatmap as a cell region of your original image. The heatmap quantifies how salient the pixels in the cell are for the chosen saliency approach.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "content": [
            {
              "inlineContent": [
                {
                  "text": "",
                  "type": "text"
                },
                {
                  "text": " ",
                  "type": "text"
                },
                {
                  "text": "Saliency is computed on individual images. While it’s possible to compute saliency on each frame of a video stream, keep in mind that the saliency of objects in the scene may vary due to subtle changes in image composition, such as image framing or new subject matter.",
                  "type": "text"
                }
              ],
              "type": "paragraph"
            }
          ],
          "name": "Note",
          "style": "note",
          "type": "aside"
        },
        {
          "inlineContent": [
            {
              "text": "If your app overlays the saliency heatmap on the original input image, upsample the heatmap and apply a colormap before showing it to the user.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "anchor": "Merge-Salient-Regions-for-Object-Based-Saliency",
          "level": 3,
          "text": "Merge Salient Regions for Object-Based Saliency",
          "type": "heading"
        },
        {
          "inlineContent": [
            {
              "text": "For object-based saliency requests, which return up to three bounding boxes, you can use either the most salient bounding box directly to crop a region of your image, or the union of all boxes. Each bounding box comes with a score that you use to rank the relevance of regions within the image.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "tabs": [
            {
              "content": [
                {
                  "code": [
                    "// Create the union of all salient regions.",
                    "var unionOfSalientRegions = CGRect(x: 0, y: 0, width: 0, height: 0)",
                    "let errorPointer = NSErrorPointer(nilLiteral: ())",
                    "let salientObjects = saliencyObservation.salientObjectsAndReturnError(errorPointer)",
                    "for salientObject in salientObjects {",
                    "    unionOfSalientRegions = unionOfSalientRegions.union(salientObject.boundingBox)",
                    "}",
                    "self.salientRect = VNImageRectForNormalizedRect(unionOfSalientRegions,",
                    "                             originalImage.extent.size.width,",
                    "                             originalImage.extent.size.height)"
                  ],
                  "syntax": "swift",
                  "type": "codeListing"
                }
              ],
              "title": "Swift"
            },
            {
              "content": [
                {
                  "code": [
                    "// Create the union of all salient regions.",
                    "CGRect unionOfSalientRegions = CGRectNull;",
                    "for(VNRectangleObservation* salientObject in boundingBoxes)",
                    "{",
                    "    unionOfSalientRegions = CGRectUnion(unionOfSalientRegions, salientObject.boundingBox);",
                    "}        ",
                    "weakSelf.salientRect = VNImageRectForNormalizedRect(unionOfSalientRegions, ",
                    "                                                    originalImage.extent.size.width, ",
                    "                                                    originalImage.extent.size.height);"
                  ],
                  "syntax": "objc",
                  "type": "codeListing"
                }
              ],
              "title": "Objective-C"
            }
          ],
          "type": "tabNavigator"
        },
        {
          "inlineContent": [
            {
              "text": "If the object-based saliency map value is close to zero everywhere, the image contains nothing that’s salient.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "anchor": "Crop-the-Image",
          "level": 3,
          "text": "Crop the Image",
          "type": "heading"
        },
        {
          "inlineContent": [
            {
              "text": "Attention-based saliency requests return only one bounding box, which you use directly to crop the image and drop uninteresting content. Use this bounding box to find the region on which to focus:",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "tabs": [
            {
              "content": [
                {
                  "code": [
                    "DispatchQueue.global(qos: .userInitiated).async {",
                    "    let croppedImage = originalImage.cropped(to: salientRect)",
                    "    let thumbnail =  UIImage(ciImage: croppedImage)",
                    "    DispatchQueue.main.async {",
                    "        self.imageView.image = thumbnail",
                    "    }",
                    "}"
                  ],
                  "syntax": "swift",
                  "type": "codeListing"
                }
              ],
              "title": "Swift"
            },
            {
              "content": [
                {
                  "code": [
                    "dispatch_async(dispatch_get_main_queue(), ^{",
                    "    CIImage* croppedImage = [originalImage imageByCroppingToRect: self.salientRect];",
                    "    UIImage* thumbnail =  [UIImage imageWithCIImage: croppedImage];",
                    "    self.imageView.image = thumbnail;",
                    "});"
                  ],
                  "syntax": "objc",
                  "type": "codeListing"
                }
              ],
              "title": "Objective-C"
            }
          ],
          "type": "tabNavigator"
        },
        {
          "inlineContent": [
            {
              "text": "If nothing is salient, the attention heatmap that’s returned highlights the central part of the image. This behavior reflects the fact that people tend to look at the center of an image if nothing in particular stands out to them.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "anchor": "Feed-Saliency-Results-into-Other-Vision-Requests",
          "level": 3,
          "text": "Feed Saliency Results into Other Vision Requests",
          "type": "heading"
        },
        {
          "inlineContent": [
            {
              "text": "Once you know which regions of an image are interesting, you can use the output of a saliency request as the input to another Vision request, like text recognition. The bounding boxes from saliency requests also help you localize the regions you’d like to search, so you can prioritize recognition on the most salient parts of an image, like signs or posters.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "tabs": [
            {
              "content": [
                {
                  "code": [
                    "let requestHandler = VNImageRequestHandler(url: imageURL, options: [:])",
                    "",
                    "let request: VNRequest",
                    "switch type {",
                    "case .attentionBased:",
                    "    request = VNGenerateAttentionBasedSaliencyImageRequest()",
                    "case .objectnessBased:",
                    "    request = VNGenerateObjectnessBasedSaliencyImageRequest()",
                    "}",
                    "try? requestHandler.perform([request])",
                    "    ",
                    "return request.results?.first as? VNSaliencyImageObservation"
                  ],
                  "syntax": "swift",
                  "type": "codeListing"
                }
              ],
              "title": "Swift"
            },
            {
              "content": [
                {
                  "code": [
                    "NSError* error;",
                    "VNImageRequestHandler* requestHandler = [[VNImageRequestHandler alloc] initWithURL:fileURL ",
                    "                                                                       options:@{}];",
                    "[imageRequestHandler performRequests:@[request] error:&error];",
                    "",
                    "return request.results.first;"
                  ],
                  "syntax": "objc",
                  "type": "codeListing"
                }
              ],
              "title": "Objective-C"
            }
          ],
          "type": "tabNavigator"
        }
      ],
      "kind": "content"
    }
  ],
  "references": {
    "doc://com.apple.documentation/documentation/technologies": {
      "abstract": [
        {
          "text": "",
          "type": "text"
        }
      ],
      "identifier": "doc://com.apple.documentation/documentation/technologies",
      "kind": "technologies",
      "role": "overview",
      "title": "Technologies",
      "type": "topic",
      "url": "/documentation/technologies"
    },
    "doc://com.apple.vision/documentation/Vision": {
      "abstract": [
        {
          "text": "Apply computer vision algorithms to perform a variety of tasks on input images and videos.",
          "type": "text"
        }
      ],
      "identifier": "doc://com.apple.vision/documentation/Vision",
      "kind": "symbol",
      "role": "collection",
      "title": "Vision",
      "type": "topic",
      "url": "/documentation/vision"
    },
    "doc://com.apple.vision/documentation/Vision/VNGenerateAttentionBasedSaliencyImageRequest": {
      "abstract": [
        {
          "text": "An object that produces a heat map that identifies the parts of an image most likely to draw attention.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "class"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "VNGenerateAttentionBasedSaliencyImageRequest"
        }
      ],
      "identifier": "doc://com.apple.vision/documentation/Vision/VNGenerateAttentionBasedSaliencyImageRequest",
      "kind": "symbol",
      "navigatorTitle": [
        {
          "kind": "identifier",
          "text": "VNGenerateAttentionBasedSaliencyImageRequest"
        }
      ],
      "role": "symbol",
      "title": "VNGenerateAttentionBasedSaliencyImageRequest",
      "type": "topic",
      "url": "/documentation/vision/vngenerateattentionbasedsaliencyimagerequest"
    },
    "doc://com.apple.vision/documentation/Vision/VNGenerateObjectnessBasedSaliencyImageRequest": {
      "abstract": [
        {
          "text": "A request that generates a heat map that identifies the parts of an image most likely to represent objects.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "class"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "VNGenerateObjectnessBasedSaliencyImageRequest"
        }
      ],
      "identifier": "doc://com.apple.vision/documentation/Vision/VNGenerateObjectnessBasedSaliencyImageRequest",
      "kind": "symbol",
      "navigatorTitle": [
        {
          "kind": "identifier",
          "text": "VNGenerateObjectnessBasedSaliencyImageRequest"
        }
      ],
      "role": "symbol",
      "title": "VNGenerateObjectnessBasedSaliencyImageRequest",
      "type": "topic",
      "url": "/documentation/vision/vngenerateobjectnessbasedsaliencyimagerequest"
    },
    "doc://com.apple.vision/documentation/Vision/VNSaliencyImageObservation": {
      "abstract": [
        {
          "text": "An observation that contains a grayscale heat map of important areas across an image.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "class"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "VNSaliencyImageObservation"
        }
      ],
      "identifier": "doc://com.apple.vision/documentation/Vision/VNSaliencyImageObservation",
      "kind": "symbol",
      "navigatorTitle": [
        {
          "kind": "identifier",
          "text": "VNSaliencyImageObservation"
        }
      ],
      "role": "symbol",
      "title": "VNSaliencyImageObservation",
      "type": "topic",
      "url": "/documentation/vision/vnsaliencyimageobservation"
    },
    "doc://com.apple.vision/documentation/Vision/highlighting-areas-of-interest-in-an-image-using-saliency": {
      "abstract": [
        {
          "text": "Quantify and visualize where people are likely to look in an image.",
          "type": "text"
        }
      ],
      "identifier": "doc://com.apple.vision/documentation/Vision/highlighting-areas-of-interest-in-an-image-using-saliency",
      "kind": "article",
      "role": "sampleCode",
      "title": "Highlighting Areas of Interest in an Image Using Saliency",
      "type": "topic",
      "url": "/documentation/vision/highlighting-areas-of-interest-in-an-image-using-saliency"
    },
    "doc://com.apple.vision/documentation/Vision/original-objective-c-and-swift-api": {
      "abstract": [],
      "identifier": "doc://com.apple.vision/documentation/Vision/original-objective-c-and-swift-api",
      "kind": "article",
      "role": "collectionGroup",
      "title": "Original Objective-C and Swift API",
      "type": "topic",
      "url": "/documentation/vision/original-objective-c-and-swift-api"
    }
  },
  "schemaVersion": {
    "major": 0,
    "minor": 3,
    "patch": 0
  },
  "sections": [],
  "seeAlsoSections": [
    {
      "anchor": "Saliency-analysis",
      "generated": true,
      "identifiers": [
        "doc://com.apple.vision/documentation/Vision/highlighting-areas-of-interest-in-an-image-using-saliency",
        "doc://com.apple.vision/documentation/Vision/VNGenerateAttentionBasedSaliencyImageRequest",
        "doc://com.apple.vision/documentation/Vision/VNGenerateObjectnessBasedSaliencyImageRequest",
        "doc://com.apple.vision/documentation/Vision/VNSaliencyImageObservation"
      ],
      "title": "Saliency analysis"
    }
  ],
  "variantOverrides": [
    {
      "patch": [
        {
          "op": "replace",
          "path": "/identifier/interfaceLanguage",
          "value": "occ"
        },
        {
          "op": "add",
          "path": "/topicSections",
          "value": null
        },
        {
          "op": "replace",
          "path": "/seeAlsoSections",
          "value": [
            {
              "anchor": "Saliency-analysis",
              "generated": true,
              "identifiers": [
                "doc://com.apple.vision/documentation/Vision/VNGenerateAttentionBasedSaliencyImageRequest",
                "doc://com.apple.vision/documentation/Vision/VNGenerateObjectnessBasedSaliencyImageRequest",
                "doc://com.apple.vision/documentation/Vision/VNSaliencyImageObservation"
              ],
              "title": "Saliency analysis"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.vision~1documentation~1Vision~1VNGenerateAttentionBasedSaliencyImageRequest/title",
          "value": "VNGenerateAttentionBasedSaliencyImageRequest"
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.vision~1documentation~1Vision~1VNGenerateAttentionBasedSaliencyImageRequest/fragments",
          "value": [
            {
              "kind": "identifier",
              "text": "VNGenerateAttentionBasedSaliencyImageRequest"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.vision~1documentation~1Vision~1VNGenerateAttentionBasedSaliencyImageRequest/navigatorTitle",
          "value": [
            {
              "kind": "identifier",
              "text": "VNGenerateAttentionBasedSaliencyImageRequest"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.vision~1documentation~1Vision~1VNGenerateObjectnessBasedSaliencyImageRequest/title",
          "value": "VNGenerateObjectnessBasedSaliencyImageRequest"
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.vision~1documentation~1Vision~1VNGenerateObjectnessBasedSaliencyImageRequest/fragments",
          "value": [
            {
              "kind": "identifier",
              "text": "VNGenerateObjectnessBasedSaliencyImageRequest"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.vision~1documentation~1Vision~1VNGenerateObjectnessBasedSaliencyImageRequest/navigatorTitle",
          "value": [
            {
              "kind": "identifier",
              "text": "VNGenerateObjectnessBasedSaliencyImageRequest"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.vision~1documentation~1Vision~1VNSaliencyImageObservation/title",
          "value": "VNSaliencyImageObservation"
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.vision~1documentation~1Vision~1VNSaliencyImageObservation/fragments",
          "value": [
            {
              "kind": "identifier",
              "text": "VNSaliencyImageObservation"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.vision~1documentation~1Vision~1VNSaliencyImageObservation/navigatorTitle",
          "value": [
            {
              "kind": "identifier",
              "text": "VNSaliencyImageObservation"
            }
          ]
        }
      ],
      "traits": [
        {
          "interfaceLanguage": "occ"
        }
      ]
    }
  ],
  "variants": [
    {
      "paths": [
        "/documentation/vision/cropping-images-using-saliency"
      ],
      "traits": [
        {
          "interfaceLanguage": "swift"
        }
      ]
    },
    {
      "paths": [
        "/documentation/vision/cropping-images-using-saliency"
      ],
      "traits": [
        {
          "interfaceLanguage": "occ"
        }
      ]
    }
  ]
}

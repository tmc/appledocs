{
  "abstract": [
    {
      "text": "Identify the most visually pleasing frames in a video by using the image-aesthetics scores request.",
      "type": "text"
    }
  ],
  "hierarchy": {
    "paths": [
      [
        "doc://com.apple.documentation/documentation/technologies",
        "doc://com.apple.vision/documentation/Vision"
      ]
    ]
  },
  "identifier": {
    "interfaceLanguage": "swift",
    "url": "doc://com.apple.vision/documentation/Vision/generating-thumbnails-from-videos"
  },
  "kind": "article",
  "legalNotices": {
    "copyright": "Copyright &copy; 2025 Apple Inc. All rights reserved.",
    "privacyPolicy": "https://www.apple.com/privacy/privacy-policy",
    "termsOfUse": "https://www.apple.com/legal/internet-services/terms/site.html"
  },
  "metadata": {
    "images": [
      {
        "identifier": "generating-high-quality-thumbnails-from-videos.png",
        "type": "card"
      }
    ],
    "modules": [
      {
        "name": "Vision"
      }
    ],
    "platforms": [
      {
        "beta": false,
        "introducedAt": "18.0",
        "name": "iOS"
      },
      {
        "beta": false,
        "introducedAt": "18.0",
        "name": "iPadOS"
      },
      {
        "beta": false,
        "introducedAt": "15.0",
        "name": "macOS"
      },
      {
        "beta": false,
        "introducedAt": "2.0",
        "name": "visionOS"
      },
      {
        "beta": false,
        "introducedAt": "16.0",
        "name": "Xcode"
      }
    ],
    "role": "sampleCode",
    "roleHeading": "Sample Code",
    "title": "Generating high-quality thumbnails from videos"
  },
  "primaryContentSections": [
    {
      "content": [
        {
          "anchor": "Overview",
          "level": 2,
          "text": "Overview",
          "type": "heading"
        },
        {
          "inlineContent": [
            {
              "text": "Using the ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.documentation/documentation/Vision",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " framework, you can process images and videos frame by frame, enabling image-analysis requests through ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.documentation/documentation/Vision/VisionRequest",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": ". The sample demonstrates how to process video input to identify the three frames with the highest aesthetic scores. By leveraging this capability, you can automatically select the best frames for creating promotional materials or for highlighting short films. The following image shows a preview of the sample:",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "identifier": "sample-thumbnail-generator-1-main-view",
              "type": "image"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "The sample uses a ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.documentation/documentation/Vision/VideoProcessor",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " to convert the video into a stream of frames, which ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.documentation/documentation/Vision/CalculateImageAestheticsScoresRequest",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " analyzes to rate each frame with an overall aesthetic score. To avoid selecting similar thumbnails, the ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.documentation/documentation/Vision/GenerateImageFeaturePrintRequest",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " compares the image similarity. Finally, ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.documentation/documentation/AVFoundation",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " extracts the images from the selected frames and presents them as thumbnails.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "anchor": "Set-up-the-representation-of-frames-and-thumbnails",
          "level": 3,
          "text": "Set up the representation of frames and thumbnails",
          "type": "heading"
        },
        {
          "inlineContent": [
            {
              "text": "The ",
              "type": "text"
            },
            {
              "code": "Frame",
              "type": "codeVoice"
            },
            {
              "text": " structure contains the information about a specific frame in the video that the sample uses to determine the best frame to use as a thumbnail. It accepts a ",
              "type": "text"
            },
            {
              "code": "CMTime",
              "type": "codeVoice"
            },
            {
              "text": " to represent the timestamp of the frame, a ",
              "type": "text"
            },
            {
              "code": "Float",
              "type": "codeVoice"
            },
            {
              "text": " value for the overall score of the frame, and a ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.documentation/documentation/Vision/FeaturePrintObservation",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " to enable comparison to other frames:",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "code": [
            "struct Frame {",
            "    /// The timestamp of the frame.",
            "    let time: CMTime",
            "",
            "    /// The score of the frame.",
            "    let score: Float",
            "",
            "    /// The feature-print observation of the frame.",
            "    let observation: FeaturePrintObservation",
            "}"
          ],
          "syntax": "swift",
          "type": "codeListing"
        },
        {
          "inlineContent": [
            {
              "text": "To present the results of each frame, the sample creates a ",
              "type": "text"
            },
            {
              "code": "Thumbnail",
              "type": "codeVoice"
            },
            {
              "text": " class that conforms to the ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.documentation/documentation/Swift/Identifiable",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " protocal. This ensures that the class has a unique identity for ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.documentation/documentation/SwiftUI/ForEach",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " to display the results. This class accepts a ",
              "type": "text"
            },
            {
              "code": "CGImage",
              "type": "codeVoice"
            },
            {
              "text": " to store the image of the frame, and a ",
              "type": "text"
            },
            {
              "code": "Frame",
              "type": "codeVoice"
            },
            {
              "text": " to establish a connection between the frame and the thumbnail:",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "code": [
            "class Thumbnail: Identifiable {",
            "    /// The image that captures from the video frame.",
            "    let image: CGImage",
            "",
            "    /// The frame that the thumbnail represents.",
            "    let frame: Frame",
            "",
            "    // ...",
            "}"
          ],
          "syntax": "swift",
          "type": "codeListing"
        },
        {
          "anchor": "Process-the-video",
          "level": 3,
          "text": "Process the video",
          "type": "heading"
        },
        {
          "inlineContent": [
            {
              "text": "The sample processes videos frame by frame using the ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.documentation/documentation/Vision/VideoProcessor",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": ". The sample first initializes the video processor with the video URL, then create the instances of the requests the sample uses to process the video:",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "code": [
            "func processVideo(for videoURL: URL, progression: Binding<Float>) async -> [Thumbnail] {",
            "    /// The instance of the `VideoProcessor` with the local path to the video file.",
            "    let videoProcessor = VideoProcessor(videoURL)",
            "",
            "    /// The request to calculate the aesthetics score for each frame.",
            "    let aestheticsScoresRequest = CalculateImageAestheticsScoresRequest()",
            "",
            "    /// The request to generate feature prints from an image.",
            "    let imageFeaturePrintRequest = GenerateImageFeaturePrintRequest()",
            "",
            "    /// The array to store information for the frames with the highest scores.",
            "    var topFrames: [Frame] = []",
            "",
            "    // ...",
            "}"
          ],
          "syntax": "swift",
          "type": "codeListing"
        },
        {
          "inlineContent": [
            {
              "text": "The ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.documentation/documentation/Vision/CalculateImageAestheticsScoresRequest",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " calculates the aesthetic scores for each frame. To ensure that the thumbnail results represent different scenes rather than variations of the same frame, ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.documentation/documentation/Vision/GenerateImageFeaturePrintRequest",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " computes the similarity between the frames.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "The sample calculates a time interval for the video processor to process approximately 100 frames, adds the ",
              "type": "text"
            },
            {
              "code": "aestheticsScoresRequest",
              "type": "codeVoice"
            },
            {
              "text": " and ",
              "type": "text"
            },
            {
              "code": "imageFeaturePrintRequest",
              "type": "codeVoice"
            },
            {
              "text": " to the video processor, then starts the video-analysis process. To store the timestamp and the results from the video-processor stream, the sample creates two dictionaries: ",
              "type": "text"
            },
            {
              "code": "aestheticsResults",
              "type": "codeVoice"
            },
            {
              "text": " and ",
              "type": "text"
            },
            {
              "code": "featurePrintResults",
              "type": "codeVoice"
            },
            {
              "text": ":",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "code": [
            "do {",
            "    /// The time interval for the video-processing cadence.",
            "    let interval = CMTime(",
            "        seconds: totalDuration / framesToEval,",
            "        preferredTimescale: timeScale",
            "    )",
            "",
            "    /// The video-processing cadence to process only 100 frames.",
            "    let cadence = VideoProcessor.Cadence.timeInterval(interval)",
            "",
            "    /// The stream that adds the aesthetics scores request to the video processor.",
            "    let aestheticsScoreStream = try await videoProcessor.addRequest(aestheticsScoresRequest, cadence: cadence)",
            "",
            "    /// The stream that adds the image feature-print request to the video processor.",
            "    let featurePrintStream = try await videoProcessor.addRequest(imageFeaturePrintRequest, cadence: cadence)",
            "    ",
            "    // Start to analyze the video.",
            "    videoProcessor.startAnalysis()",
            "",
            "    /// The dictionary to store the timestamp and the aesthetics score.",
            "    var aestheticsResults: [CMTime: Float] = [:]",
            "",
            "    /// The dictionary to store the timestamp and the feature-print observation.",
            "    var featurePrintResults: [CMTime: FeaturePrintObservation] = [:]",
            "",
            "    // ...",
            "",
            "    // Solve for the top-rated frames.",
            "    topFrames = await calculateTopFrames(aestheticsResults: aestheticsResults, featurePrintResults: featurePrintResults)",
            "} "
          ],
          "syntax": "swift",
          "type": "codeListing"
        },
        {
          "inlineContent": [
            {
              "text": "When the sample receives the results for both requests, call ",
              "type": "text"
            },
            {
              "code": "calculateTopFrames(aestheticsResults:featurePrintResults:)",
              "type": "codeVoice"
            },
            {
              "text": " to solve for the top-rated frames.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "anchor": "Solve-for-the-top-rated-frames",
          "level": 3,
          "text": "Solve for the top-rated frames",
          "type": "heading"
        },
        {
          "inlineContent": [
            {
              "text": "The function uses ",
              "type": "text"
            },
            {
              "code": "aestheticsResults",
              "type": "codeVoice"
            },
            {
              "text": " and ",
              "type": "text"
            },
            {
              "code": "featurePrintResults",
              "type": "codeVoice"
            },
            {
              "text": " to identify three frames that have the highest aesthetic scores and are different from each other:",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "code": [
            "for (time, score) in aestheticsResults {",
            "    /// The `FeaturePrintObservation` for the timestamp.",
            "    guard let featurePrint = featurePrintResults[time] else { continue }",
            "",
            "    /// The new frame at that timestamp.",
            "    let newFrame = Frame(time: time, score: score, observation: featurePrint)",
            "",
            "    /// The variable that tracks whether to add the image based on image similarity.",
            "    var isSimilar = false",
            "",
            "    // Iterate through the current top-rated frames to check whether any of them",
            "    // are similar to the new frame and find the insertion index.",
            "    for (index, frame) in topFrames.enumerated() {",
            "        if let distance = try? featurePrint.distance(to: frame.observation), distance < similarityThreshold {",
            "            // Replace the frame if the new frame has a higher score.",
            "            if newFrame.score > frame.score {",
            "                topFrames[index] = newFrame",
            "            }",
            "            isSimilar = true",
            "            break",
            "        }",
            "",
            "        // ...",
            "    }",
            "}"
          ],
          "syntax": "swift",
          "type": "codeListing"
        },
        {
          "inlineContent": [
            {
              "text": "For each result, the sample creates a new frame based on the timestamp and attaches the score and ",
              "type": "text"
            },
            {
              "code": "FeaturePrintObservation",
              "type": "codeVoice"
            },
            {
              "text": ". It checks for similar frames in ",
              "type": "text"
            },
            {
              "code": "topFrames",
              "type": "codeVoice"
            },
            {
              "text": " with the ",
              "type": "text"
            },
            {
              "code": "distance(to:)",
              "type": "codeVoice"
            },
            {
              "text": " function to compare the observations. If there is a match, the sample keeps the frame with the higher score and exits the loop.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        }
      ],
      "kind": "content"
    }
  ],
  "references": {
    "b503c5875476/GeneratingHighQualityThumbnailsFromVideos.zip": {
      "checksum": "b503c58754768f03e65760e8b1736b05aaa8b9914eac8b0b15291cc7c5e91f1c968ec394c965fee3e23c29a589f0858fbdae8f1ad3c63b68e9b950ffcd577c1e",
      "identifier": "b503c5875476/GeneratingHighQualityThumbnailsFromVideos.zip",
      "type": "download",
      "url": "https://docs-assets.developer.apple.com/published/b503c5875476/GeneratingHighQualityThumbnailsFromVideos.zip"
    },
    "doc://com.apple.documentation/documentation/AVFoundation": {
      "abstract": [
        {
          "text": "Work with audiovisual assets, control device cameras, process audio, and configure system audio interactions.",
          "type": "text"
        }
      ],
      "identifier": "doc://com.apple.documentation/documentation/AVFoundation",
      "kind": "symbol",
      "role": "collection",
      "title": "AVFoundation",
      "type": "topic",
      "url": "/documentation/AVFoundation"
    },
    "doc://com.apple.documentation/documentation/Swift/Identifiable": {
      "abstract": [
        {
          "text": "A class of types whose instances hold the value of an entity with stable identity.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "protocol"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "Identifiable"
        },
        {
          "kind": "text",
          "text": "<"
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "s:s12IdentifiableP2IDQa",
          "text": "ID"
        },
        {
          "kind": "text",
          "text": ">"
        }
      ],
      "identifier": "doc://com.apple.documentation/documentation/Swift/Identifiable",
      "kind": "symbol",
      "role": "symbol",
      "title": "Identifiable",
      "type": "topic",
      "url": "/documentation/Swift/Identifiable"
    },
    "doc://com.apple.documentation/documentation/SwiftUI/ForEach": {
      "abstract": [
        {
          "text": "A structure that computes views on demand from an underlying collection of identified data.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "struct"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "ForEach"
        },
        {
          "kind": "text",
          "text": "<"
        },
        {
          "kind": "genericParameter",
          "text": "Data"
        },
        {
          "kind": "text",
          "text": ", "
        },
        {
          "kind": "genericParameter",
          "text": "ID"
        },
        {
          "kind": "text",
          "text": ", "
        },
        {
          "kind": "genericParameter",
          "text": "Content"
        },
        {
          "kind": "text",
          "text": "> "
        },
        {
          "kind": "keyword",
          "text": "where"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "typeIdentifier",
          "text": "Data"
        },
        {
          "kind": "text",
          "text": " : "
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "s:Sk",
          "text": "RandomAccessCollection"
        },
        {
          "kind": "text",
          "text": ", "
        },
        {
          "kind": "typeIdentifier",
          "text": "ID"
        },
        {
          "kind": "text",
          "text": " : "
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "s:SH",
          "text": "Hashable"
        }
      ],
      "identifier": "doc://com.apple.documentation/documentation/SwiftUI/ForEach",
      "kind": "symbol",
      "role": "symbol",
      "title": "ForEach",
      "type": "topic",
      "url": "/documentation/SwiftUI/ForEach"
    },
    "doc://com.apple.documentation/documentation/Vision": {
      "abstract": [
        {
          "text": "Apply computer vision algorithms to perform a variety of tasks on input images and videos.",
          "type": "text"
        }
      ],
      "identifier": "doc://com.apple.documentation/documentation/Vision",
      "kind": "symbol",
      "role": "collection",
      "title": "Vision",
      "type": "topic",
      "url": "/documentation/Vision"
    },
    "doc://com.apple.documentation/documentation/Vision/CalculateImageAestheticsScoresRequest": {
      "abstract": [
        {
          "text": "A request that analyzes an image for aesthetically pleasing attributes.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "struct"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "CalculateImageAestheticsScoresRequest"
        }
      ],
      "identifier": "doc://com.apple.documentation/documentation/Vision/CalculateImageAestheticsScoresRequest",
      "kind": "symbol",
      "role": "symbol",
      "title": "CalculateImageAestheticsScoresRequest",
      "type": "topic",
      "url": "/documentation/Vision/CalculateImageAestheticsScoresRequest"
    },
    "doc://com.apple.documentation/documentation/Vision/FeaturePrintObservation": {
      "abstract": [
        {
          "text": "An observation that provides the recognized feature print.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "struct"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "FeaturePrintObservation"
        }
      ],
      "identifier": "doc://com.apple.documentation/documentation/Vision/FeaturePrintObservation",
      "kind": "symbol",
      "role": "symbol",
      "title": "FeaturePrintObservation",
      "type": "topic",
      "url": "/documentation/Vision/FeaturePrintObservation"
    },
    "doc://com.apple.documentation/documentation/Vision/GenerateImageFeaturePrintRequest": {
      "abstract": [
        {
          "text": "An image-based request to generate feature prints from an image.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "struct"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "GenerateImageFeaturePrintRequest"
        }
      ],
      "identifier": "doc://com.apple.documentation/documentation/Vision/GenerateImageFeaturePrintRequest",
      "kind": "symbol",
      "role": "symbol",
      "title": "GenerateImageFeaturePrintRequest",
      "type": "topic",
      "url": "/documentation/Vision/GenerateImageFeaturePrintRequest"
    },
    "doc://com.apple.documentation/documentation/Vision/VideoProcessor": {
      "abstract": [
        {
          "text": "An object that performs offline analysis of video content.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "final"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "keyword",
          "text": "class"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "VideoProcessor"
        }
      ],
      "identifier": "doc://com.apple.documentation/documentation/Vision/VideoProcessor",
      "kind": "symbol",
      "role": "symbol",
      "title": "VideoProcessor",
      "type": "topic",
      "url": "/documentation/Vision/VideoProcessor"
    },
    "doc://com.apple.documentation/documentation/Vision/VisionRequest": {
      "abstract": [
        {
          "text": "A type for image-analysis requests.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "protocol"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "VisionRequest"
        },
        {
          "kind": "text",
          "text": " : "
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "s:s23CustomStringConvertibleP",
          "text": "CustomStringConvertible"
        },
        {
          "kind": "text",
          "text": ", "
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "s:SH",
          "text": "Hashable"
        },
        {
          "kind": "text",
          "text": ", "
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "s:s8SendableP",
          "text": "Sendable"
        }
      ],
      "identifier": "doc://com.apple.documentation/documentation/Vision/VisionRequest",
      "kind": "symbol",
      "role": "symbol",
      "title": "VisionRequest",
      "type": "topic",
      "url": "/documentation/Vision/VisionRequest"
    },
    "doc://com.apple.documentation/documentation/technologies": {
      "abstract": [
        {
          "text": "",
          "type": "text"
        }
      ],
      "identifier": "doc://com.apple.documentation/documentation/technologies",
      "kind": "technologies",
      "role": "overview",
      "title": "Technologies",
      "type": "topic",
      "url": "/documentation/technologies"
    },
    "doc://com.apple.vision/documentation/Vision": {
      "abstract": [
        {
          "text": "Apply computer vision algorithms to perform a variety of tasks on input images and videos.",
          "type": "text"
        }
      ],
      "identifier": "doc://com.apple.vision/documentation/Vision",
      "kind": "symbol",
      "role": "collection",
      "title": "Vision",
      "type": "topic",
      "url": "/documentation/vision"
    },
    "doc://com.apple.vision/documentation/Vision/CalculateImageAestheticsScoresRequest": {
      "abstract": [
        {
          "text": "A request that analyzes an image for aesthetically pleasing attributes.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "struct"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "CalculateImageAestheticsScoresRequest"
        }
      ],
      "identifier": "doc://com.apple.vision/documentation/Vision/CalculateImageAestheticsScoresRequest",
      "kind": "symbol",
      "navigatorTitle": [
        {
          "kind": "identifier",
          "text": "CalculateImageAestheticsScoresRequest"
        }
      ],
      "role": "symbol",
      "title": "CalculateImageAestheticsScoresRequest",
      "type": "topic",
      "url": "/documentation/vision/calculateimageaestheticsscoresrequest"
    },
    "generating-high-quality-thumbnails-from-videos.png": {
      "alt": null,
      "identifier": "generating-high-quality-thumbnails-from-videos.png",
      "type": "image",
      "variants": [
        {
          "traits": [
            "2x",
            "light"
          ],
          "url": "https://docs-assets.developer.apple.com/published/2d83290e817f816074bd3c1fd3c71587/generating-high-quality-thumbnails-from-videos@2x.png"
        },
        {
          "traits": [
            "2x",
            "dark"
          ],
          "url": "https://docs-assets.developer.apple.com/published/7e7bf40054d0a653d2884c14c95ac4b6/generating-high-quality-thumbnails-from-videos~dark@2x.png"
        }
      ]
    },
    "sample-thumbnail-generator-1-main-view": {
      "alt": "A screen recording of a macOS app that demonstrates selecting a video file to generate the best thumbnails.",
      "identifier": "sample-thumbnail-generator-1-main-view",
      "type": "image",
      "variants": [
        {
          "traits": [
            "1x",
            "light"
          ],
          "url": "https://docs-assets.developer.apple.com/published/864613acf2af54ee865a04b0174615ea/sample-thumbnail-generator-1-main-view.png"
        }
      ]
    }
  },
  "sampleCodeDownload": {
    "action": {
      "identifier": "b503c5875476/GeneratingHighQualityThumbnailsFromVideos.zip",
      "isActive": true,
      "overridingTitle": "Download",
      "type": "reference"
    },
    "kind": "sampleDownload"
  },
  "schemaVersion": {
    "major": 0,
    "minor": 3,
    "patch": 0
  },
  "sections": [],
  "seeAlsoSections": [
    {
      "anchor": "Image-aesthetics-analysis",
      "generated": true,
      "identifiers": [
        "doc://com.apple.vision/documentation/Vision/CalculateImageAestheticsScoresRequest"
      ],
      "title": "Image aesthetics analysis"
    }
  ],
  "variants": [
    {
      "paths": [
        "/documentation/vision/generating-thumbnails-from-videos"
      ],
      "traits": [
        {
          "interfaceLanguage": "swift"
        }
      ]
    }
  ]
}

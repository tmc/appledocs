{
  "abstract": [
    {
      "text": "Detect faces in a front-camera AR experience, overlay virtual content, and animate facial expressions in real-time.",
      "type": "text"
    }
  ],
  "hierarchy": {
    "paths": [
      [
        "doc://com.apple.documentation/documentation/technologies",
        "doc://com.apple.arkit/documentation/ARKit",
        "doc://com.apple.arkit/documentation/ARKit/arkit-in-ios",
        "doc://com.apple.arkit/documentation/ARKit/content-anchors"
      ],
      [
        "doc://com.apple.documentation/documentation/technologies",
        "doc://com.apple.arkit/documentation/ARKit",
        "doc://com.apple.arkit/documentation/ARKit/arkit-in-ios",
        "doc://com.apple.arkit/documentation/ARKit/data-management"
      ]
    ]
  },
  "identifier": {
    "interfaceLanguage": "swift",
    "url": "doc://com.apple.arkit/documentation/ARKit/tracking-and-visualizing-faces"
  },
  "kind": "article",
  "legalNotices": {
    "copyright": "Copyright &copy; 2025 Apple Inc. All rights reserved.",
    "privacyPolicy": "https://www.apple.com/privacy/privacy-policy",
    "termsOfUse": "https://www.apple.com/legal/internet-services/terms/site.html"
  },
  "metadata": {
    "modules": [
      {
        "name": "ARKit"
      }
    ],
    "platforms": [
      {
        "beta": false,
        "introducedAt": "11.0",
        "name": "iOS"
      },
      {
        "beta": false,
        "introducedAt": "11.0",
        "name": "iPadOS"
      },
      {
        "beta": false,
        "introducedAt": "16.1",
        "name": "Xcode"
      }
    ],
    "role": "sampleCode",
    "roleHeading": "Sample Code",
    "title": "Tracking and visualizing faces"
  },
  "primaryContentSections": [
    {
      "content": [
        {
          "anchor": "Overview",
          "level": 2,
          "text": "Overview",
          "type": "heading"
        },
        {
          "inlineContent": [
            {
              "text": "This sample app presents a simple interface allowing you to choose between five augmented reality (AR) visualizations on devices with a TrueDepth front-facing camera.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "items": [
            {
              "content": [
                {
                  "inlineContent": [
                    {
                      "text": "An overlay of x/y/z axes indicating the ARKit coordinate system tracking the face (and in iOS 12, the position and orientation of each eye).",
                      "type": "text"
                    }
                  ],
                  "type": "paragraph"
                }
              ]
            },
            {
              "content": [
                {
                  "inlineContent": [
                    {
                      "text": "The face mesh provided by ARKit, showing automatic estimation of the real-world directional lighting environment, as well as a texture you can use to map 2D imagery onto the face.",
                      "type": "text"
                    }
                  ],
                  "type": "paragraph"
                }
              ]
            },
            {
              "content": [
                {
                  "inlineContent": [
                    {
                      "text": "Virtual 3D content that appears to attach to (and interact with) the user’s real face.",
                      "type": "text"
                    }
                  ],
                  "type": "paragraph"
                }
              ]
            },
            {
              "content": [
                {
                  "inlineContent": [
                    {
                      "text": "Live camera video texture-mapped onto the ARKit face mesh, with which you can create effects that appear to distort the user’s real face in 3D.",
                      "type": "text"
                    }
                  ],
                  "type": "paragraph"
                }
              ]
            },
            {
              "content": [
                {
                  "inlineContent": [
                    {
                      "text": "A simple robot character whose facial expression animates to match that of the user, showing how to use ARKit’s animation blend shape values to create experiences like the system Animoji app.",
                      "type": "text"
                    }
                  ],
                  "type": "paragraph"
                }
              ]
            }
          ],
          "type": "unorderedList"
        },
        {
          "inlineContent": [
            {
              "text": "Use the tab bar to switch between these modes.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "identifier": "FaceExampleModes",
              "type": "image"
            }
          ],
          "type": "paragraph"
        },
        {
          "content": [
            {
              "inlineContent": [
                {
                  "text": "Face tracking is available on all devices with Apple Neural Engine in iOS 14 & iPad OS 14, and requires a device with TrueDepth camera on iOS 13 & iPadOS 13 and below. ARKit is not available in iOS Simulator.",
                  "type": "text"
                }
              ],
              "type": "paragraph"
            }
          ],
          "name": "Important",
          "style": "important",
          "type": "aside"
        },
        {
          "anchor": "Start-a-face-tracking-session-in-a-scenekit-view",
          "level": 3,
          "text": "Start a face-tracking session in a scenekit view",
          "type": "heading"
        },
        {
          "inlineContent": [
            {
              "text": "Like other uses of ARKit, face tracking requires configuring and running a session (an ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.arkit/documentation/ARKit/ARSession",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " object) and rendering the camera image together with virtual content in a view. This sample uses ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.arkit/documentation/ARKit/ARSCNView",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " to display 3D content with SceneKit, but you can also use SpriteKit or build your own renderer using Metal (see ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.arkit/documentation/ARKit/ARSKView",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " and ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.arkit/documentation/ARKit/displaying-an-ar-experience-with-metal",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": ".",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "Face tracking differs from other uses of ARKit in the class you use to configure the session. To enable face tracking, create an instance of ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.arkit/documentation/ARKit/ARFaceTrackingConfiguration",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": ", configure its properties, and pass it to the ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.arkit/documentation/ARKit/ARSession/run(_:options:)",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " method of the AR session associated with your view, as shown here:",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "code": [
            "guard ARFaceTrackingConfiguration.isSupported else { return }",
            "let configuration = ARFaceTrackingConfiguration()",
            "if #available(iOS 13.0, *) {",
            "    configuration.maximumNumberOfTrackedFaces = ARFaceTrackingConfiguration.supportedNumberOfTrackedFaces",
            "}",
            "configuration.isLightEstimationEnabled = true",
            "sceneView.session.run(configuration, options: [.resetTracking, .removeExistingAnchors])"
          ],
          "syntax": "swift",
          "type": "codeListing"
        },
        {
          "anchor": "Track-the-Position-and-Orientation-of-a-Face",
          "level": 3,
          "text": "Track the Position and Orientation of a Face",
          "type": "heading"
        },
        {
          "inlineContent": [
            {
              "text": "When face tracking is active, ARKit automatically adds",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.arkit/documentation/ARKit/ARFaceAnchor",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " objects to the running AR session, containing information about the user’s face, including its position and orientation. (ARKit detects and provides information about only face at a time. If multiple faces are present in the camera image, ARKit chooses the largest or most clearly recognizable face.)",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "In a SceneKit-based AR experience, you can add 3D content corresponding to a face anchor in the ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.arkit/documentation/ARKit/ARSCNViewDelegate/renderer(_:nodeFor:)",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " or ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.arkit/documentation/ARKit/ARSCNViewDelegate/renderer(_:didAdd:for:)",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " delegate method. ARKit manages a SceneKit node for the anchor, and updates that node’s position and orientation on each frame, so any SceneKit content you add to that node automatically follows the position and orientation of the user’s face.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "code": [
            "func renderer(_ renderer: SCNSceneRenderer, nodeFor anchor: ARAnchor) -> SCNNode? {",
            "    // This class adds AR content only for face anchors.",
            "    guard anchor is ARFaceAnchor else { return nil }",
            "    ",
            "    // Load an asset from the app bundle to provide visual content for the anchor.",
            "    contentNode = SCNReferenceNode(named: \"coordinateOrigin\")",
            "    ",
            "    // Add content for eye tracking in iOS 12.",
            "    self.addEyeTransformNodes()",
            "    ",
            "    // Provide the node to ARKit for keeping in sync with the face anchor.",
            "    return contentNode",
            "}"
          ],
          "syntax": "swift",
          "type": "codeListing"
        },
        {
          "inlineContent": [
            {
              "text": "This example uses a convenience extension on ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.documentation/documentation/SceneKit/SCNReferenceNode",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " to load content from an ",
              "type": "text"
            },
            {
              "code": ".scn",
              "type": "codeVoice"
            },
            {
              "text": " file in the app bundle. The ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.arkit/documentation/ARKit/ARSCNViewDelegate/renderer(_:nodeFor:)",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " method provides that node to ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.arkit/documentation/ARKit/ARSCNView",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": ", allowing ARKit to automatically adjust the node’s position and orientation to match the tracked face.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "anchor": "Use-face-geometry-to-model-the-users-face",
          "level": 3,
          "text": "Use face geometry to model the user’s face",
          "type": "heading"
        },
        {
          "inlineContent": [
            {
              "text": "ARKit provides a coarse 3D mesh geometry matching the size, shape, topology, and current facial expression of the user’s face. ARKit also provides the ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.arkit/documentation/ARKit/ARSCNFaceGeometry",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " class, offering an easy way to visualize this mesh in SceneKit.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "Your AR experience can use this mesh to place or draw content that appears to attach to the face. For example, by applying a semitransparent texture to this geometry you could paint virtual tattoos or makeup onto the user’s skin.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "To create a SceneKit face geometry, initialize an ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.arkit/documentation/ARKit/ARSCNFaceGeometry",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " object with the Metal device your SceneKit view uses for rendering, and assign that geometry to the SceneKit node tracking the face anchor.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "code": [
            "func renderer(_ renderer: SCNSceneRenderer, nodeFor anchor: ARAnchor) -> SCNNode? {",
            "    guard let sceneView = renderer as? ARSCNView,",
            "        anchor is ARFaceAnchor else { return nil }",
            "    ",
            "    #if targetEnvironment(simulator)",
            "    #error(\"ARKit is not supported in iOS Simulator. Connect a physical iOS device and select it as your Xcode run destination, or select Generic iOS Device as a build-only destination.\") // swiftlint:disable:this line_length",
            "    #else",
            "    let faceGeometry = ARSCNFaceGeometry(device: sceneView.device!)!",
            "    let material = faceGeometry.firstMaterial!",
            "    ",
            "    material.diffuse.contents = #imageLiteral(resourceName: \"wireframeTexture\") // Example texture map image.",
            "    material.lightingModel = .physicallyBased",
            "    ",
            "    contentNode = SCNNode(geometry: faceGeometry)",
            "    #endif",
            "    return contentNode",
            "}"
          ],
          "syntax": "swift",
          "type": "codeListing"
        },
        {
          "content": [
            {
              "inlineContent": [
                {
                  "text": "This example uses a texture with transparency to create the illusion of colorful grid lines painted onto a real face. You can use the ",
                  "type": "text"
                },
                {
                  "code": "wireframeTexture.png",
                  "type": "codeVoice"
                },
                {
                  "text": " image included with this sample code project as a starting point to design your own face textures.",
                  "type": "text"
                }
              ],
              "type": "paragraph"
            }
          ],
          "name": "Note",
          "style": "note",
          "type": "aside"
        },
        {
          "inlineContent": [
            {
              "text": "ARKit updates its face mesh conform to the shape of the user’s face, even as the user blinks, talks, and makes various expressions. To make the displayed face model follow the user’s expressions, retrieve an updated face meshes in the ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.arkit/documentation/ARKit/ARSCNViewDelegate/renderer(_:didUpdate:for:)",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " delegate callback, then update the ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.arkit/documentation/ARKit/ARSCNFaceGeometry",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " object in your scene to match by passing the new face mesh to its ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.arkit/documentation/ARKit/ARSCNFaceGeometry/update(from:)",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " method:",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "code": [
            "func renderer(_ renderer: SCNSceneRenderer, didUpdate node: SCNNode, for anchor: ARAnchor) {",
            "    guard let faceGeometry = node.geometry as? ARSCNFaceGeometry,",
            "        let faceAnchor = anchor as? ARFaceAnchor",
            "        else { return }",
            "    ",
            "    faceGeometry.update(from: faceAnchor.geometry)",
            "}"
          ],
          "syntax": "swift",
          "type": "codeListing"
        },
        {
          "anchor": "Place-3D-content-on-the-users-face",
          "level": 3,
          "text": "Place 3D content on the user’s face",
          "type": "heading"
        },
        {
          "inlineContent": [
            {
              "text": "Another use of the face mesh that ARKit provides is to create ",
              "type": "text"
            },
            {
              "inlineContent": [
                {
                  "text": "occlusion geometry",
                  "type": "text"
                }
              ],
              "type": "emphasis"
            },
            {
              "text": " in your scene. An occlusion geometry is a 3D model that doesn’t render any visible content (allowing the camera image to show through), but obstructs the camera’s view of other virtual content in the scene.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "This technique creates the illusion that the real face interacts with virtual objects, even though the face is a 2D camera image and the virtual content is a rendered 3D object. For example, if you place an occlusion geometry and virtual glasses on the user’s face, the face can obscure the frame of the glasses.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "To create an occlusion geometry for the face, start by creating an ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.arkit/documentation/ARKit/ARSCNFaceGeometry",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " object as in the previous example. However, instead of configuring that object’s SceneKit material with a visible appearance, set the material to render depth but not color during rendering:",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "code": [
            "let faceGeometry = ARSCNFaceGeometry(device: sceneView.device!)!",
            "faceGeometry.firstMaterial!.colorBufferWriteMask = []",
            "occlusionNode = SCNNode(geometry: faceGeometry)",
            "occlusionNode.renderingOrder = -1"
          ],
          "syntax": "swift",
          "type": "codeListing"
        },
        {
          "inlineContent": [
            {
              "text": "Because the material renders depth, other objects rendered by SceneKit correctly appear in front of it or behind it. But because the material doesn’t render color, the camera image appears in its place.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "The sample app combines this technique with a SceneKit object positioned in front of the user’s eyes, creating an effect where the user’s nose realistically obscures the object. This object uses physically-based materials, so it automatically benefits from the real-time directional lighting information that ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.arkit/documentation/ARKit/ARFaceTrackingConfiguration",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " provides.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "content": [
            {
              "inlineContent": [
                {
                  "text": "The ",
                  "type": "text"
                },
                {
                  "code": "ARFaceGeometry.obj",
                  "type": "codeVoice"
                },
                {
                  "text": " file included in this sample project represents ARKit’s face geometry in a neutral pose. You can use this as a template to design your own 3D art assets for placement on a real face.",
                  "type": "text"
                }
              ],
              "type": "paragraph"
            }
          ],
          "name": "Note",
          "style": "note",
          "type": "aside"
        },
        {
          "anchor": "Map-camera-video-onto-3D-face-geometry",
          "level": 3,
          "text": "Map camera video onto 3D face geometry",
          "type": "heading"
        },
        {
          "inlineContent": [
            {
              "text": "For additional creative uses of face tracking, you can texture-map the live 2D video feed from the camera onto the 3D geometry that ARKit provides. After mapping pixels in the camera video onto the corresponding points on ARKit’s face mesh, you can modify that mesh, creating illusions such as resizing or distorting the user’s face in 3D.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "First, create an ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.arkit/documentation/ARKit/ARSCNFaceGeometry",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " for the face and assign the camera image to its main material. ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.arkit/documentation/ARKit/ARSCNView",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " automatically sets the scene’s ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.documentation/documentation/SceneKit/SCNScene/background",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " material to use the live video feed from the camera, so you can set the geometry to use the same material.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "code": [
            "// Show video texture as the diffuse material and disable lighting.",
            "let faceGeometry = ARSCNFaceGeometry(device: sceneView.device!, fillMesh: true)!",
            "let material = faceGeometry.firstMaterial!",
            "material.diffuse.contents = sceneView.scene.background.contents",
            "material.lightingModel = .constant"
          ],
          "syntax": "swift",
          "type": "codeListing"
        },
        {
          "inlineContent": [
            {
              "text": "To correctly align the camera image to the face, you’ll also need to modify the texture coordinates that SceneKit uses for rendering the image on the geometry. One easy way to perform this mapping is with a SceneKit shader modifier (see the ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.documentation/documentation/SceneKit/SCNShadable",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " protocol). The shader code here applies the coordinate system transformations needed to convert each vertex position in the mesh from 3D scene space to the 2D image space used by the video texture:",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "code": [
            "// Transform the vertex to the camera coordinate system.",
            "float4 vertexCamera = scn_node.modelViewTransform * _geometry.position;",
            "",
            "// Camera projection and perspective divide to get normalized viewport coordinates (clip space).",
            "float4 vertexClipSpace = scn_frame.projectionTransform * vertexCamera;",
            "vertexClipSpace /= vertexClipSpace.w;",
            "",
            "// XY in clip space is [-1,1]x[-1,1], so adjust to UV texture coordinates: [0,1]x[0,1].",
            "// Image coordinates are Y-flipped (upper-left origin).",
            "float4 vertexImageSpace = float4(vertexClipSpace.xy * 0.5 + 0.5, 0.0, 1.0);",
            "vertexImageSpace.y = 1.0 - vertexImageSpace.y;",
            "",
            "// Apply ARKit's display transform (device orientation * front-facing camera flip).",
            "float4 transformedVertex = displayTransform * vertexImageSpace;",
            "",
            "// Output as texture coordinates for use in later rendering stages.",
            "_geometry.texcoords[0] = transformedVertex.xy;"
          ],
          "syntax": "metal",
          "type": "codeListing"
        },
        {
          "inlineContent": [
            {
              "text": "When you assign a shader code string to the ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.documentation/documentation/scenekit/scnshadermodifierentrypoint/1524108-geometry",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " entry point, SceneKit configures its renderer to automatically run that code on the GPU for each vertex in the mesh. This shader code also needs to know the intended orientation for the camera image, so the sample gets that from the ARKit ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.arkit/documentation/ARKit/ARFrame/displayTransform(for:viewportSize:)",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " method and passes it to the shader’s ",
              "type": "text"
            },
            {
              "code": "displayTransform",
              "type": "codeVoice"
            },
            {
              "text": " argument:",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "code": [
            "// Pass view-appropriate image transform to the shader modifier so",
            "// that the mapped video lines up correctly with the background video.",
            "let affineTransform = frame.displayTransform(for: .portrait, viewportSize: sceneView.bounds.size)",
            "let transform = SCNMatrix4(affineTransform)",
            "faceGeometry.setValue(SCNMatrix4Invert(transform), forKey: \"displayTransform\")"
          ],
          "syntax": "swift",
          "type": "codeListing"
        },
        {
          "content": [
            {
              "inlineContent": [
                {
                  "text": "This example’s shader modifier also applies a constant scale factor to all vertices, causing the user’s face to appear larger than life. Try other transformations to distort the face in other ways.",
                  "type": "text"
                }
              ],
              "type": "paragraph"
            }
          ],
          "name": "Note",
          "style": "note",
          "type": "aside"
        },
        {
          "anchor": "Animate-a-character-with-blend-shapes",
          "level": 3,
          "text": "Animate a character with blend shapes",
          "type": "heading"
        },
        {
          "inlineContent": [
            {
              "text": "In addition to the face mesh shown in the earlier examples, ARKit also provides a more abstract representation of the user’s facial expressions. You can use this representation (called ",
              "type": "text"
            },
            {
              "inlineContent": [
                {
                  "text": "blend shapes",
                  "type": "text"
                }
              ],
              "type": "emphasis"
            },
            {
              "text": ") to control animation parameters for your own 2D or 3D assets, creating a character that follows the user’s real facial movements and expressions.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "As a basic demonstration of blend shape animation, this sample includes a simple model of a robot character’s head, created using SceneKit primitive shapes. (See the ",
              "type": "text"
            },
            {
              "code": "robotHead.scn",
              "type": "codeVoice"
            },
            {
              "text": " file in the source code.)",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "To get the user’s current facial expression, read the ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.arkit/documentation/ARKit/ARFaceAnchor/blendShapes",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " dictionary from the face anchor in the ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.arkit/documentation/ARKit/ARSCNViewDelegate/renderer(_:didUpdate:for:)",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " delegate callback. Then, examine the key-value pairs in that dictionary to calculate animation parameters for your 3D content and update that content accordingly.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "code": [
            "let blendShapes = faceAnchor.blendShapes",
            "guard let eyeBlinkLeft = blendShapes[.eyeBlinkLeft] as? Float,",
            "    let eyeBlinkRight = blendShapes[.eyeBlinkRight] as? Float,",
            "    let jawOpen = blendShapes[.jawOpen] as? Float",
            "    else { return }",
            "eyeLeftNode.scale.z = 1 - eyeBlinkLeft",
            "eyeRightNode.scale.z = 1 - eyeBlinkRight",
            "jawNode.position.y = originalJawY - jawHeight * jawOpen"
          ],
          "syntax": "swift",
          "type": "codeListing"
        },
        {
          "inlineContent": [
            {
              "text": "There are more than 50 unique ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.arkit/documentation/ARKit/ARFaceAnchor/BlendShapeLocation",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " coefficients, of which your app can use as few or as many as necessary to create the artistic effect you want. In this sample, the ",
              "type": "text"
            },
            {
              "code": "BlendShapeCharacter",
              "type": "codeVoice"
            },
            {
              "text": " class performs this calculation, mapping the ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.arkit/documentation/ARKit/ARFaceAnchor/BlendShapeLocation/eyeBlinkLeft",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " and ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.arkit/documentation/ARKit/ARFaceAnchor/BlendShapeLocation/eyeBlinkRight",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " parameters to one axis of the ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.documentation/documentation/SceneKit/SCNNode/scale",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " factor of the robot’s eyes, and the ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.arkit/documentation/ARKit/ARFaceAnchor/BlendShapeLocation/jawOpen",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " parameter to offset the position of the robot’s jaw.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        }
      ],
      "kind": "content"
    }
  ],
  "references": {
    "82453f2306c7/TrackingAndVisualizingFaces.zip": {
      "checksum": "82453f2306c7200b8690f8f84766a872d9779a9ce8646ed1d6f8101d96a0ddd9dfe0ca87d66e114f42efa1dfa814a67e88bc0813e6d776851fed2313de893c27",
      "identifier": "82453f2306c7/TrackingAndVisualizingFaces.zip",
      "type": "download",
      "url": "https://docs-assets.developer.apple.com/published/82453f2306c7/TrackingAndVisualizingFaces.zip"
    },
    "FaceExampleModes": {
      "alt": "Screenshot of UI for choosing AR face modes.",
      "identifier": "FaceExampleModes",
      "type": "image",
      "variants": [
        {
          "traits": [
            "1x",
            "light"
          ],
          "url": "https://docs-assets.developer.apple.com/published/ad7ee8f029463c1094b9d67248c1529e/FaceExampleModes.png"
        }
      ]
    },
    "doc://com.apple.arkit/documentation/ARKit": {
      "abstract": [
        {
          "text": "Integrate hardware sensing features to produce augmented reality apps and games.",
          "type": "text"
        }
      ],
      "identifier": "doc://com.apple.arkit/documentation/ARKit",
      "kind": "symbol",
      "role": "collection",
      "title": "ARKit",
      "type": "topic",
      "url": "/documentation/arkit"
    },
    "doc://com.apple.arkit/documentation/ARKit/ARFaceAnchor": {
      "abstract": [
        {
          "text": "An anchor for a unique face that is visible in the front-facing camera.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "class"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "ARFaceAnchor"
        }
      ],
      "identifier": "doc://com.apple.arkit/documentation/ARKit/ARFaceAnchor",
      "kind": "symbol",
      "navigatorTitle": [
        {
          "kind": "identifier",
          "text": "ARFaceAnchor"
        }
      ],
      "role": "symbol",
      "title": "ARFaceAnchor",
      "type": "topic",
      "url": "/documentation/arkit/arfaceanchor"
    },
    "doc://com.apple.arkit/documentation/ARKit/ARFaceAnchor/BlendShapeLocation": {
      "abstract": [
        {
          "text": "Identifiers for specific facial features, for use with coefficients describing the relative movements of those features.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "struct"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "BlendShapeLocation"
        }
      ],
      "identifier": "doc://com.apple.arkit/documentation/ARKit/ARFaceAnchor/BlendShapeLocation",
      "kind": "symbol",
      "navigatorTitle": [
        {
          "kind": "identifier",
          "text": "BlendShapeLocation"
        }
      ],
      "role": "symbol",
      "title": "ARFaceAnchor.BlendShapeLocation",
      "type": "topic",
      "url": "/documentation/arkit/arfaceanchor/blendshapelocation"
    },
    "doc://com.apple.arkit/documentation/ARKit/ARFaceAnchor/BlendShapeLocation/eyeBlinkLeft": {
      "abstract": [
        {
          "text": "The coefficient describing closure of the eyelids over the left eye.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "static"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "keyword",
          "text": "let"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "eyeBlinkLeft"
        },
        {
          "kind": "text",
          "text": ": "
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "c:objc(cs)ARFaceAnchor",
          "text": "ARFaceAnchor"
        },
        {
          "kind": "text",
          "text": "."
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "c:@T@ARBlendShapeLocation",
          "text": "BlendShapeLocation"
        }
      ],
      "identifier": "doc://com.apple.arkit/documentation/ARKit/ARFaceAnchor/BlendShapeLocation/eyeBlinkLeft",
      "kind": "symbol",
      "navigatorTitle": [
        {
          "kind": "identifier",
          "text": "ARBlendShapeLocationEyeBlinkLeft"
        }
      ],
      "role": "symbol",
      "title": "eyeBlinkLeft",
      "type": "topic",
      "url": "/documentation/arkit/arfaceanchor/blendshapelocation/eyeblinkleft"
    },
    "doc://com.apple.arkit/documentation/ARKit/ARFaceAnchor/BlendShapeLocation/eyeBlinkRight": {
      "abstract": [
        {
          "text": "The coefficient describing closure of the eyelids over the right eye.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "static"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "keyword",
          "text": "let"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "eyeBlinkRight"
        },
        {
          "kind": "text",
          "text": ": "
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "c:objc(cs)ARFaceAnchor",
          "text": "ARFaceAnchor"
        },
        {
          "kind": "text",
          "text": "."
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "c:@T@ARBlendShapeLocation",
          "text": "BlendShapeLocation"
        }
      ],
      "identifier": "doc://com.apple.arkit/documentation/ARKit/ARFaceAnchor/BlendShapeLocation/eyeBlinkRight",
      "kind": "symbol",
      "navigatorTitle": [
        {
          "kind": "identifier",
          "text": "ARBlendShapeLocationEyeBlinkRight"
        }
      ],
      "role": "symbol",
      "title": "eyeBlinkRight",
      "type": "topic",
      "url": "/documentation/arkit/arfaceanchor/blendshapelocation/eyeblinkright"
    },
    "doc://com.apple.arkit/documentation/ARKit/ARFaceAnchor/BlendShapeLocation/jawOpen": {
      "abstract": [
        {
          "text": "The coefficient describing an opening of the lower jaw.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "static"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "keyword",
          "text": "let"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "jawOpen"
        },
        {
          "kind": "text",
          "text": ": "
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "c:objc(cs)ARFaceAnchor",
          "text": "ARFaceAnchor"
        },
        {
          "kind": "text",
          "text": "."
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "c:@T@ARBlendShapeLocation",
          "text": "BlendShapeLocation"
        }
      ],
      "identifier": "doc://com.apple.arkit/documentation/ARKit/ARFaceAnchor/BlendShapeLocation/jawOpen",
      "kind": "symbol",
      "navigatorTitle": [
        {
          "kind": "identifier",
          "text": "ARBlendShapeLocationJawOpen"
        }
      ],
      "role": "symbol",
      "title": "jawOpen",
      "type": "topic",
      "url": "/documentation/arkit/arfaceanchor/blendshapelocation/jawopen"
    },
    "doc://com.apple.arkit/documentation/ARKit/ARFaceAnchor/blendShapes": {
      "abstract": [
        {
          "text": "A dictionary of named coefficients representing the detected facial expression in terms of the movement of specific facial features.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "var"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "blendShapes"
        },
        {
          "kind": "text",
          "text": ": ["
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "c:objc(cs)ARFaceAnchor",
          "text": "ARFaceAnchor"
        },
        {
          "kind": "text",
          "text": "."
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "c:@T@ARBlendShapeLocation",
          "text": "BlendShapeLocation"
        },
        {
          "kind": "text",
          "text": " : "
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "c:objc(cs)NSNumber",
          "text": "NSNumber"
        },
        {
          "kind": "text",
          "text": "]"
        }
      ],
      "identifier": "doc://com.apple.arkit/documentation/ARKit/ARFaceAnchor/blendShapes",
      "kind": "symbol",
      "navigatorTitle": [
        {
          "kind": "identifier",
          "text": "blendShapes"
        }
      ],
      "role": "symbol",
      "title": "blendShapes",
      "type": "topic",
      "url": "/documentation/arkit/arfaceanchor/blendshapes"
    },
    "doc://com.apple.arkit/documentation/ARKit/ARFaceTrackingConfiguration": {
      "abstract": [
        {
          "text": "A configuration that tracks facial movement and expressions using the front camera.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "class"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "ARFaceTrackingConfiguration"
        }
      ],
      "identifier": "doc://com.apple.arkit/documentation/ARKit/ARFaceTrackingConfiguration",
      "kind": "symbol",
      "navigatorTitle": [
        {
          "kind": "identifier",
          "text": "ARFaceTrackingConfiguration"
        }
      ],
      "role": "symbol",
      "title": "ARFaceTrackingConfiguration",
      "type": "topic",
      "url": "/documentation/arkit/arfacetrackingconfiguration"
    },
    "doc://com.apple.arkit/documentation/ARKit/ARFrame/displayTransform(for:viewportSize:)": {
      "abstract": [
        {
          "text": "Returns an affine transform for converting between normalized image coordinates and a coordinate space appropriate for rendering the camera image onscreen.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "func"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "displayTransform"
        },
        {
          "kind": "text",
          "text": "("
        },
        {
          "kind": "externalParam",
          "text": "for"
        },
        {
          "kind": "text",
          "text": ": "
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "c:@E@UIInterfaceOrientation",
          "text": "UIInterfaceOrientation"
        },
        {
          "kind": "text",
          "text": ", "
        },
        {
          "kind": "externalParam",
          "text": "viewportSize"
        },
        {
          "kind": "text",
          "text": ": "
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "c:@S@CGSize",
          "text": "CGSize"
        },
        {
          "kind": "text",
          "text": ") -> "
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "c:@S@CGAffineTransform",
          "text": "CGAffineTransform"
        }
      ],
      "identifier": "doc://com.apple.arkit/documentation/ARKit/ARFrame/displayTransform(for:viewportSize:)",
      "kind": "symbol",
      "navigatorTitle": [
        {
          "kind": "identifier",
          "text": "displayTransformForOrientation:viewportSize:"
        }
      ],
      "role": "symbol",
      "title": "displayTransform(for:viewportSize:)",
      "type": "topic",
      "url": "/documentation/arkit/arframe/displaytransform(for:viewportsize:)"
    },
    "doc://com.apple.arkit/documentation/ARKit/ARSCNFaceGeometry": {
      "abstract": [
        {
          "text": "A SceneKit representation of face topology for use with face information that an AR session provides.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "class"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "ARSCNFaceGeometry"
        }
      ],
      "identifier": "doc://com.apple.arkit/documentation/ARKit/ARSCNFaceGeometry",
      "kind": "symbol",
      "navigatorTitle": [
        {
          "kind": "identifier",
          "text": "ARSCNFaceGeometry"
        }
      ],
      "role": "symbol",
      "title": "ARSCNFaceGeometry",
      "type": "topic",
      "url": "/documentation/arkit/arscnfacegeometry"
    },
    "doc://com.apple.arkit/documentation/ARKit/ARSCNFaceGeometry/update(from:)": {
      "abstract": [
        {
          "text": "Deforms the SceneKit geometry to match the specified face mesh.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "func"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "update"
        },
        {
          "kind": "text",
          "text": "("
        },
        {
          "kind": "externalParam",
          "text": "from"
        },
        {
          "kind": "text",
          "text": ": "
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "c:objc(cs)ARFaceGeometry",
          "text": "ARFaceGeometry"
        },
        {
          "kind": "text",
          "text": ")"
        }
      ],
      "identifier": "doc://com.apple.arkit/documentation/ARKit/ARSCNFaceGeometry/update(from:)",
      "kind": "symbol",
      "navigatorTitle": [
        {
          "kind": "identifier",
          "text": "updateFromFaceGeometry:"
        }
      ],
      "role": "symbol",
      "title": "update(from:)",
      "type": "topic",
      "url": "/documentation/arkit/arscnfacegeometry/update(from:)"
    },
    "doc://com.apple.arkit/documentation/ARKit/ARSCNView": {
      "abstract": [
        {
          "text": "A view that blends virtual 3D content from SceneKit into your augmented reality experience.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "class"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "ARSCNView"
        }
      ],
      "identifier": "doc://com.apple.arkit/documentation/ARKit/ARSCNView",
      "kind": "symbol",
      "navigatorTitle": [
        {
          "kind": "identifier",
          "text": "ARSCNView"
        }
      ],
      "role": "symbol",
      "title": "ARSCNView",
      "type": "topic",
      "url": "/documentation/arkit/arscnview"
    },
    "doc://com.apple.arkit/documentation/ARKit/ARSCNViewDelegate/renderer(_:didAdd:for:)": {
      "abstract": [
        {
          "text": "Tells the delegate that a SceneKit node corresponding to a new AR anchor has been added to the scene.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "func"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "renderer"
        },
        {
          "kind": "text",
          "text": "(any "
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "c:objc(pl)SCNSceneRenderer",
          "text": "SCNSceneRenderer"
        },
        {
          "kind": "text",
          "text": ", "
        },
        {
          "kind": "externalParam",
          "text": "didAdd"
        },
        {
          "kind": "text",
          "text": ": "
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "c:objc(cs)SCNNode",
          "text": "SCNNode"
        },
        {
          "kind": "text",
          "text": ", "
        },
        {
          "kind": "externalParam",
          "text": "for"
        },
        {
          "kind": "text",
          "text": ": "
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "c:objc(cs)ARAnchor",
          "text": "ARAnchor"
        },
        {
          "kind": "text",
          "text": ")"
        }
      ],
      "identifier": "doc://com.apple.arkit/documentation/ARKit/ARSCNViewDelegate/renderer(_:didAdd:for:)",
      "kind": "symbol",
      "navigatorTitle": [
        {
          "kind": "identifier",
          "text": "renderer:didAddNode:forAnchor:"
        }
      ],
      "role": "symbol",
      "title": "renderer(_:didAdd:for:)",
      "type": "topic",
      "url": "/documentation/arkit/arscnviewdelegate/renderer(_:didadd:for:)"
    },
    "doc://com.apple.arkit/documentation/ARKit/ARSCNViewDelegate/renderer(_:didUpdate:for:)": {
      "abstract": [
        {
          "text": "Tells the delegate that a SceneKit node’s properties have been updated to match the current state of its corresponding anchor.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "func"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "renderer"
        },
        {
          "kind": "text",
          "text": "(any "
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "c:objc(pl)SCNSceneRenderer",
          "text": "SCNSceneRenderer"
        },
        {
          "kind": "text",
          "text": ", "
        },
        {
          "kind": "externalParam",
          "text": "didUpdate"
        },
        {
          "kind": "text",
          "text": ": "
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "c:objc(cs)SCNNode",
          "text": "SCNNode"
        },
        {
          "kind": "text",
          "text": ", "
        },
        {
          "kind": "externalParam",
          "text": "for"
        },
        {
          "kind": "text",
          "text": ": "
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "c:objc(cs)ARAnchor",
          "text": "ARAnchor"
        },
        {
          "kind": "text",
          "text": ")"
        }
      ],
      "identifier": "doc://com.apple.arkit/documentation/ARKit/ARSCNViewDelegate/renderer(_:didUpdate:for:)",
      "kind": "symbol",
      "navigatorTitle": [
        {
          "kind": "identifier",
          "text": "renderer:didUpdateNode:forAnchor:"
        }
      ],
      "role": "symbol",
      "title": "renderer(_:didUpdate:for:)",
      "type": "topic",
      "url": "/documentation/arkit/arscnviewdelegate/renderer(_:didupdate:for:)"
    },
    "doc://com.apple.arkit/documentation/ARKit/ARSCNViewDelegate/renderer(_:nodeFor:)": {
      "abstract": [
        {
          "text": "Asks the delegate to provide a SceneKit node corresponding to a newly added anchor.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "func"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "renderer"
        },
        {
          "kind": "text",
          "text": "(any "
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "c:objc(pl)SCNSceneRenderer",
          "text": "SCNSceneRenderer"
        },
        {
          "kind": "text",
          "text": ", "
        },
        {
          "kind": "externalParam",
          "text": "nodeFor"
        },
        {
          "kind": "text",
          "text": ": "
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "c:objc(cs)ARAnchor",
          "text": "ARAnchor"
        },
        {
          "kind": "text",
          "text": ") -> "
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "c:objc(cs)SCNNode",
          "text": "SCNNode"
        },
        {
          "kind": "text",
          "text": "?"
        }
      ],
      "identifier": "doc://com.apple.arkit/documentation/ARKit/ARSCNViewDelegate/renderer(_:nodeFor:)",
      "kind": "symbol",
      "navigatorTitle": [
        {
          "kind": "identifier",
          "text": "renderer:nodeForAnchor:"
        }
      ],
      "role": "symbol",
      "title": "renderer(_:nodeFor:)",
      "type": "topic",
      "url": "/documentation/arkit/arscnviewdelegate/renderer(_:nodefor:)"
    },
    "doc://com.apple.arkit/documentation/ARKit/ARSKView": {
      "abstract": [
        {
          "text": "A view that blends virtual 2D content from SpriteKit into the 3D space of an augmented reality experience.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "class"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "ARSKView"
        }
      ],
      "identifier": "doc://com.apple.arkit/documentation/ARKit/ARSKView",
      "kind": "symbol",
      "navigatorTitle": [
        {
          "kind": "identifier",
          "text": "ARSKView"
        }
      ],
      "role": "symbol",
      "title": "ARSKView",
      "type": "topic",
      "url": "/documentation/arkit/arskview"
    },
    "doc://com.apple.arkit/documentation/ARKit/ARSession": {
      "abstract": [
        {
          "text": "The object that manages the major tasks associated with every AR experience, such as motion tracking, camera passthrough, and image analysis.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "class"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "ARSession"
        }
      ],
      "identifier": "doc://com.apple.arkit/documentation/ARKit/ARSession",
      "kind": "symbol",
      "navigatorTitle": [
        {
          "kind": "identifier",
          "text": "ARSession"
        }
      ],
      "role": "symbol",
      "title": "ARSession",
      "type": "topic",
      "url": "/documentation/arkit/arsession"
    },
    "doc://com.apple.arkit/documentation/ARKit/ARSession/run(_:options:)": {
      "abstract": [
        {
          "text": "Starts AR processing for the session with the specified configuration and options.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "func"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "run"
        },
        {
          "kind": "text",
          "text": "("
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "c:objc(cs)ARConfiguration",
          "text": "ARConfiguration"
        },
        {
          "kind": "text",
          "text": ", "
        },
        {
          "kind": "externalParam",
          "text": "options"
        },
        {
          "kind": "text",
          "text": ": "
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "c:objc(cs)ARSession",
          "text": "ARSession"
        },
        {
          "kind": "text",
          "text": "."
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "c:@E@ARSessionRunOptions",
          "text": "RunOptions"
        },
        {
          "kind": "text",
          "text": ")"
        }
      ],
      "identifier": "doc://com.apple.arkit/documentation/ARKit/ARSession/run(_:options:)",
      "kind": "symbol",
      "navigatorTitle": [
        {
          "kind": "identifier",
          "text": "runWithConfiguration:options:"
        }
      ],
      "role": "symbol",
      "title": "run(_:options:)",
      "type": "topic",
      "url": "/documentation/arkit/arsession/run(_:options:)"
    },
    "doc://com.apple.arkit/documentation/ARKit/arkit-in-ios": {
      "abstract": [
        {
          "text": "Integrate iOS device camera and motion features to produce augmented reality experiences in your app or game.",
          "type": "text"
        }
      ],
      "identifier": "doc://com.apple.arkit/documentation/ARKit/arkit-in-ios",
      "kind": "article",
      "role": "collectionGroup",
      "title": "ARKit in iOS",
      "type": "topic",
      "url": "/documentation/arkit/arkit-in-ios"
    },
    "doc://com.apple.arkit/documentation/ARKit/combining-user-face-tracking-and-world-tracking": {
      "abstract": [
        {
          "text": "Track the user’s face in an app that displays an AR experience with the rear camera.",
          "type": "text"
        }
      ],
      "identifier": "doc://com.apple.arkit/documentation/ARKit/combining-user-face-tracking-and-world-tracking",
      "kind": "article",
      "role": "sampleCode",
      "title": "Combining user face-tracking and world tracking",
      "type": "topic",
      "url": "/documentation/arkit/combining-user-face-tracking-and-world-tracking"
    },
    "doc://com.apple.arkit/documentation/ARKit/content-anchors": {
      "abstract": [
        {
          "text": "Identify items in the physical environment, including planar surfaces, images, physical objects, body positions, and faces.",
          "type": "text"
        }
      ],
      "identifier": "doc://com.apple.arkit/documentation/ARKit/content-anchors",
      "kind": "article",
      "role": "collectionGroup",
      "title": "Content Anchors",
      "type": "topic",
      "url": "/documentation/arkit/content-anchors"
    },
    "doc://com.apple.arkit/documentation/ARKit/data-management": {
      "abstract": [
        {
          "text": "Obtain detailed information about skeletal and face geometry, and saved world data.",
          "type": "text"
        }
      ],
      "identifier": "doc://com.apple.arkit/documentation/ARKit/data-management",
      "kind": "article",
      "role": "collectionGroup",
      "title": "Data Management",
      "type": "topic",
      "url": "/documentation/arkit/data-management"
    },
    "doc://com.apple.arkit/documentation/ARKit/displaying-an-ar-experience-with-metal": {
      "abstract": [
        {
          "text": "Control rendering of your app’s virtual content on top of a camera feed.",
          "type": "text"
        }
      ],
      "identifier": "doc://com.apple.arkit/documentation/ARKit/displaying-an-ar-experience-with-metal",
      "kind": "article",
      "role": "article",
      "title": "Displaying an AR Experience with Metal",
      "type": "topic",
      "url": "/documentation/arkit/displaying-an-ar-experience-with-metal"
    },
    "doc://com.apple.documentation/documentation/SceneKit/SCNNode/scale": {
      "abstract": [
        {
          "text": "The scale factor applied to the node. Animatable.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "var"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "scale"
        },
        {
          "kind": "text",
          "text": ": "
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "c:@S@SCNVector3",
          "text": "SCNVector3"
        },
        {
          "kind": "text",
          "text": " { "
        },
        {
          "kind": "keyword",
          "text": "get"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "keyword",
          "text": "set"
        },
        {
          "kind": "text",
          "text": " }"
        }
      ],
      "identifier": "doc://com.apple.documentation/documentation/SceneKit/SCNNode/scale",
      "kind": "symbol",
      "role": "symbol",
      "title": "scale",
      "type": "topic",
      "url": "/documentation/SceneKit/SCNNode/scale"
    },
    "doc://com.apple.documentation/documentation/SceneKit/SCNReferenceNode": {
      "abstract": [
        {
          "text": "A scene graph node that serves as a placeholder for content to be loaded from a separate scene file.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "class"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "SCNReferenceNode"
        }
      ],
      "identifier": "doc://com.apple.documentation/documentation/SceneKit/SCNReferenceNode",
      "kind": "symbol",
      "role": "symbol",
      "title": "SCNReferenceNode",
      "type": "topic",
      "url": "/documentation/SceneKit/SCNReferenceNode"
    },
    "doc://com.apple.documentation/documentation/SceneKit/SCNScene/background": {
      "abstract": [
        {
          "text": "A background to be rendered before the rest of the scene.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "var"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "background"
        },
        {
          "kind": "text",
          "text": ": "
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "c:objc(cs)SCNMaterialProperty",
          "text": "SCNMaterialProperty"
        },
        {
          "kind": "text",
          "text": " { "
        },
        {
          "kind": "keyword",
          "text": "get"
        },
        {
          "kind": "text",
          "text": " }"
        }
      ],
      "identifier": "doc://com.apple.documentation/documentation/SceneKit/SCNScene/background",
      "kind": "symbol",
      "role": "symbol",
      "title": "background",
      "type": "topic",
      "url": "/documentation/SceneKit/SCNScene/background"
    },
    "doc://com.apple.documentation/documentation/SceneKit/SCNShadable": {
      "abstract": [
        {
          "text": "Methods for customizing SceneKit’s rendering of geometry and materials using Metal or OpenGL shader programs.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "protocol"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "SCNShadable"
        },
        {
          "kind": "text",
          "text": " : "
        },
        {
          "kind": "typeIdentifier",
          "preciseIdentifier": "c:objc(pl)NSObject",
          "text": "NSObjectProtocol"
        }
      ],
      "identifier": "doc://com.apple.documentation/documentation/SceneKit/SCNShadable",
      "kind": "symbol",
      "role": "symbol",
      "title": "SCNShadable",
      "type": "topic",
      "url": "/documentation/SceneKit/SCNShadable"
    },
    "doc://com.apple.documentation/documentation/scenekit/scnshadermodifierentrypoint/1524108-geometry": {
      "abstract": [
        {
          "text": "Use this entry point to deform a geometry’s surface or alter its vertex attributes.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "text",
          "text": "static let "
        },
        {
          "kind": "identifier",
          "text": "geometry"
        },
        {
          "kind": "text",
          "text": ": SCNShaderModifierEntryPoint"
        }
      ],
      "identifier": "doc://com.apple.documentation/documentation/scenekit/scnshadermodifierentrypoint/1524108-geometry",
      "kind": "symbol",
      "role": "symbol",
      "title": "geometry",
      "type": "topic",
      "url": "/documentation/scenekit/scnshadermodifierentrypoint/1524108-geometry"
    },
    "doc://com.apple.documentation/documentation/technologies": {
      "abstract": [
        {
          "text": "",
          "type": "text"
        }
      ],
      "identifier": "doc://com.apple.documentation/documentation/technologies",
      "kind": "technologies",
      "role": "overview",
      "title": "Technologies",
      "type": "topic",
      "url": "/documentation/technologies"
    }
  },
  "sampleCodeDownload": {
    "action": {
      "identifier": "82453f2306c7/TrackingAndVisualizingFaces.zip",
      "isActive": true,
      "overridingTitle": "Download",
      "type": "reference"
    },
    "kind": "sampleDownload"
  },
  "schemaVersion": {
    "major": 0,
    "minor": 3,
    "patch": 0
  },
  "sections": [],
  "seeAlsoSections": [
    {
      "anchor": "Face-Tracking",
      "generated": true,
      "identifiers": [
        "doc://com.apple.arkit/documentation/ARKit/combining-user-face-tracking-and-world-tracking",
        "doc://com.apple.arkit/documentation/ARKit/ARFaceAnchor"
      ],
      "title": "Face Tracking"
    }
  ],
  "variantOverrides": [
    {
      "patch": [
        {
          "op": "replace",
          "path": "/identifier/interfaceLanguage",
          "value": "occ"
        },
        {
          "op": "add",
          "path": "/topicSections",
          "value": null
        },
        {
          "op": "replace",
          "path": "/seeAlsoSections",
          "value": [
            {
              "anchor": "Face-Tracking",
              "generated": true,
              "identifiers": [
                "doc://com.apple.arkit/documentation/ARKit/ARFaceAnchor"
              ],
              "title": "Face Tracking"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARFaceAnchor~1BlendShapeLocation/title",
          "value": "ARBlendShapeLocation"
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARFaceAnchor~1BlendShapeLocation/fragments",
          "value": [
            {
              "kind": "identifier",
              "text": "ARBlendShapeLocation"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARFaceAnchor~1BlendShapeLocation/navigatorTitle",
          "value": [
            {
              "kind": "identifier",
              "text": "ARBlendShapeLocation"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARFaceAnchor~1BlendShapeLocation~1eyeBlinkRight/title",
          "value": "ARBlendShapeLocationEyeBlinkRight"
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARFaceAnchor~1BlendShapeLocation~1eyeBlinkRight/fragments",
          "value": [
            {
              "kind": "identifier",
              "text": "ARBlendShapeLocationEyeBlinkRight"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARFaceAnchor~1BlendShapeLocation~1eyeBlinkLeft/title",
          "value": "ARBlendShapeLocationEyeBlinkLeft"
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARFaceAnchor~1BlendShapeLocation~1eyeBlinkLeft/fragments",
          "value": [
            {
              "kind": "identifier",
              "text": "ARBlendShapeLocationEyeBlinkLeft"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARSKView/title",
          "value": "ARSKView"
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARSKView/fragments",
          "value": [
            {
              "kind": "identifier",
              "text": "ARSKView"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARSKView/navigatorTitle",
          "value": [
            {
              "kind": "identifier",
              "text": "ARSKView"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.documentation~1documentation~1scenekit~1scnshadermodifierentrypoint~11524108-geometry/title",
          "value": "SCNShaderModifierEntryPointGeometry"
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARFaceAnchor/title",
          "value": "ARFaceAnchor"
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARFaceAnchor/fragments",
          "value": [
            {
              "kind": "identifier",
              "text": "ARFaceAnchor"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARFaceAnchor/navigatorTitle",
          "value": [
            {
              "kind": "identifier",
              "text": "ARFaceAnchor"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARSession~1run(_:options:)/title",
          "value": "runWithConfiguration:options:"
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARSession~1run(_:options:)/fragments",
          "value": [
            {
              "kind": "text",
              "text": "- "
            },
            {
              "kind": "identifier",
              "text": "runWithConfiguration:options:"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.documentation~1documentation~1SceneKit~1SCNShadable/fragments",
          "value": [
            {
              "kind": "keyword",
              "text": "@protocol"
            },
            {
              "kind": "text",
              "text": " "
            },
            {
              "kind": "identifier",
              "text": "SCNShadable"
            },
            {
              "kind": "text",
              "text": " <"
            },
            {
              "kind": "typeIdentifier",
              "preciseIdentifier": "c:objc(pl)NSObject",
              "text": "NSObject"
            },
            {
              "kind": "text",
              "text": ">"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARSCNViewDelegate~1renderer(_:didUpdate:for:)/title",
          "value": "renderer:didUpdateNode:forAnchor:"
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARSCNViewDelegate~1renderer(_:didUpdate:for:)/fragments",
          "value": [
            {
              "kind": "text",
              "text": "- "
            },
            {
              "kind": "identifier",
              "text": "renderer:didUpdateNode:forAnchor:"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARSCNViewDelegate~1renderer(_:didAdd:for:)/title",
          "value": "renderer:didAddNode:forAnchor:"
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARSCNViewDelegate~1renderer(_:didAdd:for:)/fragments",
          "value": [
            {
              "kind": "text",
              "text": "- "
            },
            {
              "kind": "identifier",
              "text": "renderer:didAddNode:forAnchor:"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARFaceTrackingConfiguration/title",
          "value": "ARFaceTrackingConfiguration"
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARFaceTrackingConfiguration/fragments",
          "value": [
            {
              "kind": "identifier",
              "text": "ARFaceTrackingConfiguration"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARFaceTrackingConfiguration/navigatorTitle",
          "value": [
            {
              "kind": "identifier",
              "text": "ARFaceTrackingConfiguration"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARSCNViewDelegate~1renderer(_:nodeFor:)/title",
          "value": "renderer:nodeForAnchor:"
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARSCNViewDelegate~1renderer(_:nodeFor:)/fragments",
          "value": [
            {
              "kind": "text",
              "text": "- "
            },
            {
              "kind": "identifier",
              "text": "renderer:nodeForAnchor:"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARFaceAnchor~1blendShapes/title",
          "value": "blendShapes"
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARFaceAnchor~1blendShapes/fragments",
          "value": [
            {
              "kind": "identifier",
              "text": "blendShapes"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.documentation~1documentation~1SceneKit~1SCNReferenceNode/fragments",
          "value": [
            {
              "kind": "keyword",
              "text": "@interface"
            },
            {
              "kind": "text",
              "text": " "
            },
            {
              "kind": "identifier",
              "text": "SCNReferenceNode"
            },
            {
              "kind": "text",
              "text": " : "
            },
            {
              "kind": "typeIdentifier",
              "preciseIdentifier": "c:objc(cs)SCNNode",
              "text": "SCNNode"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARFaceAnchor~1BlendShapeLocation~1jawOpen/title",
          "value": "ARBlendShapeLocationJawOpen"
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARFaceAnchor~1BlendShapeLocation~1jawOpen/fragments",
          "value": [
            {
              "kind": "identifier",
              "text": "ARBlendShapeLocationJawOpen"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.documentation~1documentation~1SceneKit~1SCNScene~1background/fragments",
          "value": [
            {
              "kind": "keyword",
              "text": "@property"
            },
            {
              "kind": "text",
              "text": " ("
            },
            {
              "kind": "keyword",
              "text": "nonatomic"
            },
            {
              "kind": "text",
              "text": ", "
            },
            {
              "kind": "keyword",
              "text": "readonly"
            },
            {
              "kind": "text",
              "text": ") "
            },
            {
              "kind": "typeIdentifier",
              "preciseIdentifier": "c:objc(cs)SCNMaterialProperty",
              "text": "SCNMaterialProperty"
            },
            {
              "kind": "text",
              "text": " * "
            },
            {
              "kind": "identifier",
              "text": "background"
            },
            {
              "kind": "text",
              "text": ";"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARSCNFaceGeometry~1update(from:)/title",
          "value": "updateFromFaceGeometry:"
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARSCNFaceGeometry~1update(from:)/fragments",
          "value": [
            {
              "kind": "text",
              "text": "- "
            },
            {
              "kind": "identifier",
              "text": "updateFromFaceGeometry:"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARSCNView/title",
          "value": "ARSCNView"
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARSCNView/fragments",
          "value": [
            {
              "kind": "identifier",
              "text": "ARSCNView"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARSCNView/navigatorTitle",
          "value": [
            {
              "kind": "identifier",
              "text": "ARSCNView"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARFrame~1displayTransform(for:viewportSize:)/title",
          "value": "displayTransformForOrientation:viewportSize:"
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARFrame~1displayTransform(for:viewportSize:)/fragments",
          "value": [
            {
              "kind": "text",
              "text": "- "
            },
            {
              "kind": "identifier",
              "text": "displayTransformForOrientation:viewportSize:"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.documentation~1documentation~1SceneKit~1SCNNode~1scale/fragments",
          "value": [
            {
              "kind": "keyword",
              "text": "@property"
            },
            {
              "kind": "text",
              "text": " ("
            },
            {
              "kind": "keyword",
              "text": "nonatomic"
            },
            {
              "kind": "text",
              "text": ") "
            },
            {
              "kind": "typeIdentifier",
              "preciseIdentifier": "c:@S@SCNVector3",
              "text": "SCNVector3"
            },
            {
              "kind": "text",
              "text": " "
            },
            {
              "kind": "identifier",
              "text": "scale"
            },
            {
              "kind": "text",
              "text": ";"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARSession/title",
          "value": "ARSession"
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARSession/fragments",
          "value": [
            {
              "kind": "identifier",
              "text": "ARSession"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARSession/navigatorTitle",
          "value": [
            {
              "kind": "identifier",
              "text": "ARSession"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARSCNFaceGeometry/title",
          "value": "ARSCNFaceGeometry"
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARSCNFaceGeometry/fragments",
          "value": [
            {
              "kind": "identifier",
              "text": "ARSCNFaceGeometry"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.arkit~1documentation~1ARKit~1ARSCNFaceGeometry/navigatorTitle",
          "value": [
            {
              "kind": "identifier",
              "text": "ARSCNFaceGeometry"
            }
          ]
        }
      ],
      "traits": [
        {
          "interfaceLanguage": "occ"
        }
      ]
    }
  ],
  "variants": [
    {
      "paths": [
        "/documentation/arkit/tracking-and-visualizing-faces"
      ],
      "traits": [
        {
          "interfaceLanguage": "swift"
        }
      ]
    },
    {
      "paths": [
        "/documentation/arkit/tracking-and-visualizing-faces"
      ],
      "traits": [
        {
          "interfaceLanguage": "occ"
        }
      ]
    }
  ]
}

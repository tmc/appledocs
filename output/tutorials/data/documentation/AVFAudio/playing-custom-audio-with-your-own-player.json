{
  "abstract": [
    {
      "text": "Construct an audio player to play your custom audio data, and optionally take advantage of the advanced features of AirPlay 2.",
      "type": "text"
    }
  ],
  "hierarchy": {
    "paths": [
      [
        "doc://com.apple.documentation/documentation/technologies",
        "doc://com.apple.avfaudio/documentation/AVFAudio",
        "doc://com.apple.avfaudio/documentation/AVFAudio/audio-engine"
      ]
    ]
  },
  "identifier": {
    "interfaceLanguage": "swift",
    "url": "doc://com.apple.avfaudio/documentation/AVFAudio/playing-custom-audio-with-your-own-player"
  },
  "kind": "article",
  "legalNotices": {
    "copyright": "Copyright &copy; 2025 Apple Inc. All rights reserved.",
    "privacyPolicy": "https://www.apple.com/privacy/privacy-policy",
    "termsOfUse": "https://www.apple.com/legal/internet-services/terms/site.html"
  },
  "metadata": {
    "modules": [
      {
        "name": "AVFAudio"
      }
    ],
    "platforms": [
      {
        "beta": false,
        "introducedAt": "17.0",
        "name": "iOS"
      },
      {
        "beta": false,
        "introducedAt": "17.0",
        "name": "iPadOS"
      },
      {
        "beta": false,
        "introducedAt": "15.4",
        "name": "Xcode"
      }
    ],
    "role": "sampleCode",
    "roleHeading": "Sample Code",
    "title": "Playing custom audio with your own player"
  },
  "primaryContentSections": [
    {
      "content": [
        {
          "anchor": "Overview",
          "level": 2,
          "text": "Overview",
          "type": "heading"
        },
        {
          "inlineContent": [
            {
              "text": "This sample code project builds a robust audio player from the ground up, using the ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.documentation/documentation/AVFoundation/AVSampleBufferAudioRenderer",
              "isActive": true,
              "overridingTitle": "AVSampleBufferAudioRenderer",
              "overridingTitleInlineContent": [
                {
                  "code": "AVSampleBufferAudioRenderer",
                  "type": "codeVoice"
                }
              ],
              "type": "reference"
            },
            {
              "text": " and ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.documentation/documentation/AVFoundation/AVSampleBufferRenderSynchronizer",
              "isActive": true,
              "overridingTitle": "AVSampleBufferRenderSynchronizer",
              "overridingTitleInlineContent": [
                {
                  "code": "AVSampleBufferRenderSynchronizer",
                  "type": "codeVoice"
                }
              ],
              "type": "reference"
            },
            {
              "text": " classes to manage enqueuing and playback of audio that you provide. The player uses a playlist of playable items, and allows the user to edit the contents of the playlist while playback is in progress.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "The example app also uses ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.avfaudio/documentation/AVFAudio/AVAudioSession",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " to indicate that it plays long-form audio content — music, audiobooks, podcasts, or other content that a user will listen to over a substantial period of time. This allows the player to benefit from AirPlay 2. When the app plays to a compatible device such as HomePod, AirPlay 2 dramatically improves playback reliability and performance, and enables advanced features such as multiroom playback.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "anchor": "Specify-Long-Form-Audio",
          "level": 3,
          "text": "Specify Long-Form Audio",
          "type": "heading"
        },
        {
          "inlineContent": [
            {
              "text": "To use AirPlay 2 for playback to compatible output devices, configure your audio session with the ",
              "type": "text"
            },
            {
              "identifier": "doc://com.apple.avfaudio/documentation/AVFAudio/AVAudioSession/RouteSharingPolicy-swift.enum/longFormAudio",
              "isActive": true,
              "type": "reference"
            },
            {
              "text": " route-sharing policy. Typically, you do this once when your app starts up:",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "code": [
            "do {",
            "    try AVAudioSession.sharedInstance().setCategory(.playback, mode: .default, policy: .longFormAudio)",
            "} catch {",
            "    print(\"Failed to set audio session route sharing policy: \\(error)\")",
            "}"
          ],
          "syntax": "swift",
          "type": "codeListing"
        },
        {
          "inlineContent": [
            {
              "text": "The ",
              "type": "text"
            },
            {
              "code": ".longFormAudio",
              "type": "codeVoice"
            },
            {
              "text": " route-sharing policy is a hint to the system that your audio content is suitable for extended listening sessions. As a side effect, it also allows your audio to benefit from AirPlay 2 for extended buffering and improved responsiveness to commands.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "You can choose not to configure your audio as long form if your content isn’t intended to displace playback of long-form content from apps such as Apple Music, iTunes, or Podcasts.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "anchor": "Provide-Audio-Content",
          "level": 3,
          "text": "Provide Audio Content",
          "type": "heading"
        },
        {
          "inlineContent": [
            {
              "text": "For your custom audio player, start by deciding how you want to identify content to the player. One way is to manage a playlist, a persistent list of playable items. Alternatively, use a temporary queue of items, a single item, or a continuous stream of audio that has no distinct identity as a separate item.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "This sample app demonstrates the use of a playlist, each item representing a single music track. The player has public APIs that app code can use to manipulate the contents and order of the playlist, and start and stop playback. The ",
              "type": "text"
            },
            {
              "code": "SampleBufferPlayer",
              "type": "codeVoice"
            },
            {
              "text": " class implements these APIs.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "Internally, your player should contain logic to enqueue buffers in advance of their scheduled playback time, and to handle transitions between items.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "In this example project, a ",
              "type": "text"
            },
            {
              "code": "SampleBufferSerializer",
              "type": "codeVoice"
            },
            {
              "text": " object provides the enqueuing logic, using ",
              "type": "text"
            },
            {
              "code": "SampleBufferItem",
              "type": "codeVoice"
            },
            {
              "text": " objects to wrap playable items and provide audio buffers on request. ",
              "type": "text"
            },
            {
              "code": "SampleBufferItem",
              "type": "codeVoice"
            },
            {
              "text": " objects use ",
              "type": "text"
            },
            {
              "code": "SampleBufferSource",
              "type": "codeVoice"
            },
            {
              "text": " objects to provide the basic audio data.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "These classes are discussed in more detail in the following sections.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "anchor": "Manage-Your-Playlist",
          "level": 3,
          "text": "Manage Your Playlist",
          "type": "heading"
        },
        {
          "inlineContent": [
            {
              "text": "Use a ",
              "type": "text"
            },
            {
              "code": "SampleBufferPlayer",
              "type": "codeVoice"
            },
            {
              "text": " to manage a playlist through a private ",
              "type": "text"
            },
            {
              "code": "Playlist",
              "type": "codeVoice"
            },
            {
              "text": " structure:",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "code": [
            "private struct Playlist {",
            "    ",
            "    // Items in the playlist.",
            "    var items: [SampleBufferItem] = []",
            "    ",
            "    // The current item index, or nil if the player is in a stopped state.",
            "    var currentIndex: Int?",
            "    ",
            "}"
          ],
          "syntax": "swift",
          "type": "codeListing"
        },
        {
          "inlineContent": [
            {
              "text": "The ",
              "type": "text"
            },
            {
              "code": "currentItemIndex",
              "type": "codeVoice"
            },
            {
              "text": " member represents the current state of the player. It indicates which element of the ",
              "type": "text"
            },
            {
              "code": "items",
              "type": "codeVoice"
            },
            {
              "text": " array is the currently playing item, and implies a corresponding player state.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "items": [
            {
              "content": [
                {
                  "inlineContent": [
                    {
                      "text": "If ",
                      "type": "text"
                    },
                    {
                      "code": "currentItemIndex",
                      "type": "codeVoice"
                    },
                    {
                      "text": " is ",
                      "type": "text"
                    },
                    {
                      "code": "nil",
                      "type": "codeVoice"
                    },
                    {
                      "text": ", there’s no current item, and the player is in a stopped state.",
                      "type": "text"
                    }
                  ],
                  "type": "paragraph"
                }
              ]
            },
            {
              "content": [
                {
                  "inlineContent": [
                    {
                      "text": "If ",
                      "type": "text"
                    },
                    {
                      "code": "currentItemIndex",
                      "type": "codeVoice"
                    },
                    {
                      "text": " is not ",
                      "type": "text"
                    },
                    {
                      "code": "nil",
                      "type": "codeVoice"
                    },
                    {
                      "text": ", it’s a valid index into the ",
                      "type": "text"
                    },
                    {
                      "code": "items",
                      "type": "codeVoice"
                    },
                    {
                      "text": " array, and the player is in a playing or paused state.",
                      "type": "text"
                    }
                  ],
                  "type": "paragraph"
                }
              ]
            }
          ],
          "type": "unorderedList"
        },
        {
          "inlineContent": [
            {
              "text": "A number of ",
              "type": "text"
            },
            {
              "code": "SampleBufferPlayer",
              "type": "codeVoice"
            },
            {
              "text": " methods manage the ",
              "type": "text"
            },
            {
              "code": "Playlist",
              "type": "codeVoice"
            },
            {
              "text": ". For example, you use an ",
              "type": "text"
            },
            {
              "code": "insertItem",
              "type": "codeVoice"
            },
            {
              "text": " method to insert an item into the queue:",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "code": [
            "func insertItem(_ newItem: PlaylistItem, at index: Int) {",
            "    ",
            "    playbackSerializer.printLog(component: .player, message: \"inserting item at playlist#\\(index)\")",
            "    atomicitySemaphore.wait()",
            "    defer { atomicitySemaphore.signal() }",
            "",
            "    playlist.items.insert(playbackSerializer.sampleBufferItem(playlistItem: newItem, fromOffset: .zero), at: index)",
            "    ",
            "    // Adjust the current index, if necessary.",
            "    if let currentIndex = playlist.currentIndex, index <= currentIndex {",
            "        playlist.currentIndex = currentIndex + 1",
            "    }",
            "",
            "    // Let the current item continue playing.",
            "    continueWithCurrentItems()",
            "}"
          ],
          "syntax": "swift",
          "type": "codeListing"
        },
        {
          "content": [
            {
              "inlineContent": [
                {
                  "text": "All methods of  ",
                  "type": "text"
                },
                {
                  "code": "SampleBufferPlayer",
                  "type": "codeVoice"
                },
                {
                  "text": " that modify the ",
                  "type": "text"
                },
                {
                  "code": "Playlist",
                  "type": "codeVoice"
                },
                {
                  "text": " must do so in a thread-safe manner. In this example implementation, a ",
                  "type": "text"
                },
                {
                  "code": "DispatchSemaphore",
                  "type": "codeVoice"
                },
                {
                  "text": " guarantees that only one method modifies the state at a time. Such methods issue a semaphore ",
                  "type": "text"
                },
                {
                  "code": "wait",
                  "type": "codeVoice"
                },
                {
                  "text": " on entry, then use a ",
                  "type": "text"
                },
                {
                  "code": "defer",
                  "type": "codeVoice"
                },
                {
                  "text": " statement to guarantee a matching semaphore ",
                  "type": "text"
                },
                {
                  "code": "signal",
                  "type": "codeVoice"
                },
                {
                  "text": " on exit.",
                  "type": "text"
                }
              ],
              "type": "paragraph"
            }
          ],
          "name": "Important",
          "style": "important",
          "type": "aside"
        },
        {
          "inlineContent": [
            {
              "text": "In general, all of the public methods of  ",
              "type": "text"
            },
            {
              "code": "SampleBufferPlayer",
              "type": "codeVoice"
            },
            {
              "text": " end up invoking one of these helper methods: ",
              "type": "text"
            },
            {
              "code": "restartWithItems(fromIndex:atOffset:)",
              "type": "codeVoice"
            },
            {
              "text": " or ",
              "type": "text"
            },
            {
              "code": "continueWithCurrentItems()",
              "type": "codeVoice"
            },
            {
              "text": ".",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "The ",
              "type": "text"
            },
            {
              "code": "restartWithItems(fromIndex:atOffset:)",
              "type": "codeVoice"
            },
            {
              "text": " method forces playback of the currently playing item (if any) to stop before playback restarts with a new list of items:",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "code": [
            "private func restartWithItems(fromIndex proposedIndex: Int?, atOffset offset: CMTime) {",
            "    ",
            "    // Stop the player if there's no current item.",
            "    guard let currentIndex = proposedIndex,",
            "        (0 ..< playlist.items.count).contains(currentIndex) else { stopCurrentItems(); return }",
            "    ",
            "    // Start playing the requested items.",
            "    playlist.currentIndex = currentIndex",
            "    let playbackItems = Array(playlist.items[currentIndex ..< playlist.items.count])",
            "    ",
            "    playbackSerializer.restartQueue(with: playbackItems, atOffset: offset)",
            "}"
          ],
          "syntax": "swift",
          "type": "codeListing"
        },
        {
          "inlineContent": [
            {
              "text": "The ",
              "type": "text"
            },
            {
              "code": "continueWithCurrentItems()",
              "type": "codeVoice"
            },
            {
              "text": " method allows the currently playing item to continue to play, followed by a new list of items that will play after the current item finishes:",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "code": [
            "private func continueWithCurrentItems() {",
            "",
            "    // Stop the player if there's nothing to play.",
            "    guard let currentIndex = playlist.currentIndex else { stopCurrentItems(); return }",
            "",
            "    // Continue playing with a list of items to play starting from the current item.",
            "    let playbackItems = Array(playlist.items[currentIndex ..< playlist.items.count])",
            "    ",
            "    playbackSerializer.continueQueue(with: playbackItems)",
            "}"
          ],
          "syntax": "swift",
          "type": "codeListing"
        },
        {
          "inlineContent": [
            {
              "text": "Both methods begin by checking that the player state isn’t “stopped.” They then construct a queue of items to play — which may consist of fewer items than the entire playlist — and pass the queue to a corresponding method in the ",
              "type": "text"
            },
            {
              "code": "SampleBufferSerializer",
              "type": "codeVoice"
            },
            {
              "text": " class. This transfer of control takes place on a serial ",
              "type": "text"
            },
            {
              "code": "DispatchQueue",
              "type": "codeVoice"
            },
            {
              "text": ", so that the ",
              "type": "text"
            },
            {
              "code": "SampleBufferSerializer",
              "type": "codeVoice"
            },
            {
              "text": " object handles one action at a time.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "anchor": "Schedule-Playback",
          "level": 3,
          "text": "Schedule Playback",
          "type": "heading"
        },
        {
          "inlineContent": [
            {
              "text": "Once the ",
              "type": "text"
            },
            {
              "code": "SampleBufferSerializer",
              "type": "codeVoice"
            },
            {
              "text": " object receives a queue of items to play, it proceeds with the dual tasks of translating the items into a sequence of sample buffers containing audio data and enqueuing the buffers for rendering. The ",
              "type": "text"
            },
            {
              "code": "SampleBufferSerializer",
              "type": "codeVoice"
            },
            {
              "text": " causes an ",
              "type": "text"
            },
            {
              "code": "AVSampleBufferRenderSynchronizer",
              "type": "codeVoice"
            },
            {
              "text": " object to play audio at the correct time, and an ",
              "type": "text"
            },
            {
              "code": "AVSampleBufferAudioRenderer",
              "type": "codeVoice"
            },
            {
              "text": " object to render enqueued audio sample buffers in time for playback.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "content": [
            {
              "inlineContent": [
                {
                  "text": "",
                  "type": "text"
                },
                {
                  "code": "SampleBufferSerializer",
                  "type": "codeVoice"
                },
                {
                  "text": " invokes all of its internal methods via a serial ",
                  "type": "text"
                },
                {
                  "code": "DispatchQueue",
                  "type": "codeVoice"
                },
                {
                  "text": ". This ensures that changes to instance properties during each method invocation are thread-safe.",
                  "type": "text"
                }
              ],
              "type": "paragraph"
            }
          ],
          "name": "Important",
          "style": "important",
          "type": "aside"
        },
        {
          "inlineContent": [
            {
              "text": "The most important ",
              "type": "text"
            },
            {
              "code": "SampleBufferSerializer",
              "type": "codeVoice"
            },
            {
              "text": " methods are the two methods it uses to accept control from the ",
              "type": "text"
            },
            {
              "code": "SampleBufferPlayer",
              "type": "codeVoice"
            },
            {
              "text": " object: ",
              "type": "text"
            },
            {
              "code": "restartPlayback(with:atOffset:)",
              "type": "codeVoice"
            },
            {
              "text": " and ",
              "type": "text"
            },
            {
              "code": "continuePlayback(with:)",
              "type": "codeVoice"
            },
            {
              "text": ". Both methods take, as their first parameter, a queue of items to play in order. The methods differ in the way they handle the item that was previously playing, if any.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "The ",
              "type": "text"
            },
            {
              "code": "restartPlayback(with:atOffset:)",
              "type": "codeVoice"
            },
            {
              "text": " method stops any current playback, which means that the audio renderer can simply flush all enqueued buffers, and restart enqueuing from the first provided item.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "By contrast, ",
              "type": "text"
            },
            {
              "code": "continuePlayback(with:)",
              "type": "codeVoice"
            },
            {
              "text": " attempts to let the current playback continue, and allow enqueued buffers to remain enqueued, as far as possible. It does this by examining its new list of items, and finding items that match items that have already been scheduled. It can then do a partial flush of the audio renderer, starting from the playback time of the first nonmatching item, and resume enqueuing sample buffers from that point.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "The time-based partial flush uses asynchronous API with a completion handler. Upon completion, enqueuing actually restarts in an additional method, ",
              "type": "text"
            },
            {
              "code": "finishContinuePlayback(with:didFlush:)",
              "type": "codeVoice"
            },
            {
              "text": ". This division into a pair of methods is an implementation detail.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "anchor": "Enqueue-Buffers",
          "level": 3,
          "text": "Enqueue Buffers",
          "type": "heading"
        },
        {
          "inlineContent": [
            {
              "text": "As it converts its list of items into a sequence of sample buffers, the ",
              "type": "text"
            },
            {
              "code": "SampleBufferSerializer",
              "type": "codeVoice"
            },
            {
              "text": " must enqueue the buffers to an ",
              "type": "text"
            },
            {
              "code": "AVSampleBufferAudioRenderer",
              "type": "codeVoice"
            },
            {
              "text": " object. Control of enqueuing relies on the ",
              "type": "text"
            },
            {
              "code": "requestMediaDataWhenReady(on:using:)",
              "type": "codeVoice"
            },
            {
              "text": " method of ",
              "type": "text"
            },
            {
              "code": "AVSampleBufferAudioRenderer",
              "type": "codeVoice"
            },
            {
              "text": ", which takes a closure parameter that the renderer invokes whenever it’s ready for more sample buffers.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "For a single playlist item, the sequence of events is straightforward, but when there are multiple playlist items, the ",
              "type": "text"
            },
            {
              "code": "SampleBufferSerializer",
              "type": "codeVoice"
            },
            {
              "text": " may need to enqueue sample buffers from a second (or subsequent) item before the first item finishes playing. That means, usually, that enqueuing of sample buffers takes place well in advance of the playback time of those buffers. This happens as follows:",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "items": [
            {
              "content": [
                {
                  "inlineContent": [
                    {
                      "text": "The serializer queues sample buffers as early as possible, when requested by the ",
                      "type": "text"
                    },
                    {
                      "code": "AVSampleBufferAudioRenderer",
                      "type": "codeVoice"
                    },
                    {
                      "text": ".",
                      "type": "text"
                    }
                  ],
                  "type": "paragraph"
                }
              ]
            },
            {
              "content": [
                {
                  "inlineContent": [
                    {
                      "text": "After enqueuing the last sample buffer of a playlist item, the serializer places a boundary observer on the ",
                      "type": "text"
                    },
                    {
                      "code": "AVSampleBufferRenderSynchronizer",
                      "type": "codeVoice"
                    },
                    {
                      "text": " timeline, at the expected ending playback time of that item.",
                      "type": "text"
                    }
                  ],
                  "type": "paragraph"
                }
              ]
            },
            {
              "content": [
                {
                  "inlineContent": [
                    {
                      "text": "When the boundary observer fires, playback of that item is complete. The serializer discards the item, removes the boundary observer, updates the current item in the ",
                      "type": "text"
                    },
                    {
                      "code": "SampleBufferPlayer",
                      "type": "codeVoice"
                    },
                    {
                      "text": ", and generates a notification that’s used to update the current item display in the UI. It also places a periodic observer on the timeline, which fires every 0.1 seconds, to generate future timing notifications for updating the playback time display in the UI.",
                      "type": "text"
                    }
                  ],
                  "type": "paragraph"
                }
              ]
            },
            {
              "content": [
                {
                  "inlineContent": [
                    {
                      "text": "This process of placing boundary observers, and removing them when they fire, repeats as each item finishes enqueuing its buffers.",
                      "type": "text"
                    }
                  ],
                  "type": "paragraph"
                }
              ]
            }
          ],
          "type": "orderedList"
        },
        {
          "inlineContent": [
            {
              "text": "In the implementation, the important methods are ",
              "type": "text"
            },
            {
              "code": "provideMediaData()",
              "type": "codeVoice"
            },
            {
              "text": ", which enqueues sample buffers and places boundary observers, and ",
              "type": "text"
            },
            {
              "code": "updateCurrentPlayerItem(at:)",
              "type": "codeVoice"
            },
            {
              "text": ", which the boundary observer invokes to handle the transition between items.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "anchor": "Retrieve-Sample-Buffers",
          "level": 3,
          "text": "Retrieve Sample Buffers",
          "type": "heading"
        },
        {
          "inlineContent": [
            {
              "text": "A ",
              "type": "text"
            },
            {
              "code": "SampleBufferItem",
              "type": "codeVoice"
            },
            {
              "text": " object provides the ",
              "type": "text"
            },
            {
              "code": "SampleBufferSerializer",
              "type": "codeVoice"
            },
            {
              "text": " with a sequence of audio sample buffers for a playback item:",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "code": [
            "// Try to read from a sample buffer source.",
            "let source = sampleBufferSource!",
            "let sampleBuffer = try source.nextSampleBuffer()",
            "",
            "// Keep track of the actual duration of this source.",
            "endOffset = source.nextSampleOffset",
            "",
            "return sampleBuffer"
          ],
          "syntax": "swift",
          "type": "codeListing"
        },
        {
          "inlineContent": [
            {
              "text": "The ",
              "type": "text"
            },
            {
              "code": "SampleBufferItem",
              "type": "codeVoice"
            },
            {
              "text": " object also manages state associated with the playback item. This includes a ",
              "type": "text"
            },
            {
              "code": "uniqueID",
              "type": "codeVoice"
            },
            {
              "text": " property that identifies the item uniquely within the playlist, even when items share the same underlying ",
              "type": "text"
            },
            {
              "code": "PlaylistItem",
              "type": "codeVoice"
            },
            {
              "text": ".",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "It also includes an ",
              "type": "text"
            },
            {
              "code": "endOffset",
              "type": "codeVoice"
            },
            {
              "text": " property, which is the time — relative to the start of the item — when playback of buffers enqueued so far will end. This value is important for determining the placement of the boundary observers described in the previous section, on the ",
              "type": "text"
            },
            {
              "code": "AVSampleBufferRenderSynchronizer",
              "type": "codeVoice"
            },
            {
              "text": " timeline.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "A ",
              "type": "text"
            },
            {
              "code": "SampleBufferItem",
              "type": "codeVoice"
            },
            {
              "text": " object keeps a reference to the source of its audio data while the data is being enqueued. The audio data source is represented by a ",
              "type": "text"
            },
            {
              "code": "SampleBufferSource",
              "type": "codeVoice"
            },
            {
              "text": ", which is an object that you customize for your audio data.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "As soon as all data for the item is enqueued, the item can discard its data source object, allowing the system to reclaim its resources (files and memory, for example).",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "anchor": "Provide-Your-Data-Source",
          "level": 3,
          "text": "Provide Your Data Source",
          "type": "heading"
        },
        {
          "inlineContent": [
            {
              "text": "Ultimately, you’ll need to provide custom data source code to fetch your custom audio data and package it into ",
              "type": "text"
            },
            {
              "code": "CMSampleBuffer",
              "type": "codeVoice"
            },
            {
              "text": " objects that can be passed to the audio renderer.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "In this example project, a ",
              "type": "text"
            },
            {
              "code": "SampleBufferSource",
              "type": "codeVoice"
            },
            {
              "text": " object serves as the data source. It simply reads data from an audio file stored within the app bundle.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "The class also contains helper methods that convert an ",
              "type": "text"
            },
            {
              "code": "AVAudioBuffer",
              "type": "codeVoice"
            },
            {
              "text": " to a ",
              "type": "text"
            },
            {
              "code": "CMSampleBuffer",
              "type": "codeVoice"
            },
            {
              "text": ", which is the required type for data passed to the audio renderer.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "anchor": "Configure-Logging",
          "level": 3,
          "text": "Configure Logging",
          "type": "heading"
        },
        {
          "inlineContent": [
            {
              "text": "The player and serializer implementations contain detailed, formatted logging of their actions. The logging output can be crucial to understanding the behavior of the code during development and testing.",
              "type": "text"
            }
          ],
          "type": "paragraph"
        },
        {
          "inlineContent": [
            {
              "text": "By default, logging suppresses messages about the enqueuing of specific sample buffers, to avoid flooding the console. If desired, you can enable those messages by setting ",
              "type": "text"
            },
            {
              "code": "shouldLogEnqueuerMessages",
              "type": "codeVoice"
            },
            {
              "text": " to ",
              "type": "text"
            },
            {
              "code": "true",
              "type": "codeVoice"
            },
            {
              "text": ".",
              "type": "text"
            }
          ],
          "type": "paragraph"
        }
      ],
      "kind": "content"
    }
  ],
  "references": {
    "68f077a79b68/PlayingCustomAudioWithYourOwnPlayer.zip": {
      "checksum": "68f077a79b6861bcb6536d1f14f88897a8c31dca49cbf2973957369ae4b7d0d05078a6054c862709f008a1af33f7639d4141052f5ce412ba39bbf74068ef705c",
      "identifier": "68f077a79b68/PlayingCustomAudioWithYourOwnPlayer.zip",
      "type": "download",
      "url": "https://docs-assets.developer.apple.com/published/68f077a79b68/PlayingCustomAudioWithYourOwnPlayer.zip"
    },
    "doc://com.apple.avfaudio/documentation/AVFAudio": {
      "abstract": [
        {
          "text": "Play, record, and process audio; configure your app’s system audio behavior.",
          "type": "text"
        }
      ],
      "identifier": "doc://com.apple.avfaudio/documentation/AVFAudio",
      "kind": "symbol",
      "role": "collection",
      "title": "AVFAudio",
      "type": "topic",
      "url": "/documentation/avfaudio"
    },
    "doc://com.apple.avfaudio/documentation/AVFAudio/AVAudioPlayerNode": {
      "abstract": [
        {
          "text": "An object for scheduling the playback of buffers or segments of audio files.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "class"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "AVAudioPlayerNode"
        }
      ],
      "identifier": "doc://com.apple.avfaudio/documentation/AVFAudio/AVAudioPlayerNode",
      "kind": "symbol",
      "navigatorTitle": [
        {
          "kind": "identifier",
          "text": "AVAudioPlayerNode"
        }
      ],
      "role": "symbol",
      "title": "AVAudioPlayerNode",
      "type": "topic",
      "url": "/documentation/avfaudio/avaudioplayernode"
    },
    "doc://com.apple.avfaudio/documentation/AVFAudio/AVAudioSession": {
      "abstract": [
        {
          "text": "An object that communicates to the system how you intend to use audio in your app.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "class"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "AVAudioSession"
        }
      ],
      "identifier": "doc://com.apple.avfaudio/documentation/AVFAudio/AVAudioSession",
      "kind": "symbol",
      "navigatorTitle": [
        {
          "kind": "identifier",
          "text": "AVAudioSession"
        }
      ],
      "role": "symbol",
      "title": "AVAudioSession",
      "type": "topic",
      "url": "/documentation/avfaudio/avaudiosession"
    },
    "doc://com.apple.avfaudio/documentation/AVFAudio/AVAudioSession/RouteSharingPolicy-swift.enum/longFormAudio": {
      "abstract": [
        {
          "text": "A policy that routes output to the shared long-form audio output.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "case"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "longFormAudio"
        }
      ],
      "identifier": "doc://com.apple.avfaudio/documentation/AVFAudio/AVAudioSession/RouteSharingPolicy-swift.enum/longFormAudio",
      "kind": "symbol",
      "navigatorTitle": [
        {
          "kind": "identifier",
          "text": "AVAudioSessionRouteSharingPolicyLongFormAudio"
        }
      ],
      "role": "symbol",
      "title": "AVAudioSession.RouteSharingPolicy.longFormAudio",
      "type": "topic",
      "url": "/documentation/avfaudio/avaudiosession/routesharingpolicy-swift.enum/longformaudio"
    },
    "doc://com.apple.avfaudio/documentation/AVFAudio/audio-engine": {
      "abstract": [
        {
          "text": "Perform advanced real-time and offline audio processing, implement 3D spatialization, and work with MIDI and samplers.",
          "type": "text"
        }
      ],
      "identifier": "doc://com.apple.avfaudio/documentation/AVFAudio/audio-engine",
      "kind": "article",
      "role": "collectionGroup",
      "title": "Audio Engine",
      "type": "topic",
      "url": "/documentation/avfaudio/audio-engine"
    },
    "doc://com.apple.avfaudio/documentation/AVFAudio/using-voice-processing": {
      "abstract": [
        {
          "text": "Add voice-processing capabilities to your app by using audio engine.",
          "type": "text"
        }
      ],
      "identifier": "doc://com.apple.avfaudio/documentation/AVFAudio/using-voice-processing",
      "kind": "article",
      "role": "sampleCode",
      "title": "Using voice processing",
      "type": "topic",
      "url": "/documentation/avfaudio/using-voice-processing"
    },
    "doc://com.apple.documentation/documentation/AVFoundation/AVSampleBufferAudioRenderer": {
      "abstract": [
        {
          "text": "An object used to decompress audio and play compressed or uncompressed audio.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "class"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "AVSampleBufferAudioRenderer"
        }
      ],
      "identifier": "doc://com.apple.documentation/documentation/AVFoundation/AVSampleBufferAudioRenderer",
      "kind": "symbol",
      "role": "symbol",
      "title": "AVSampleBufferAudioRenderer",
      "type": "topic",
      "url": "/documentation/AVFoundation/AVSampleBufferAudioRenderer"
    },
    "doc://com.apple.documentation/documentation/AVFoundation/AVSampleBufferRenderSynchronizer": {
      "abstract": [
        {
          "text": "An object used to synchronize multiple queued sample buffers to a single timeline.",
          "type": "text"
        }
      ],
      "fragments": [
        {
          "kind": "keyword",
          "text": "class"
        },
        {
          "kind": "text",
          "text": " "
        },
        {
          "kind": "identifier",
          "text": "AVSampleBufferRenderSynchronizer"
        }
      ],
      "identifier": "doc://com.apple.documentation/documentation/AVFoundation/AVSampleBufferRenderSynchronizer",
      "kind": "symbol",
      "role": "symbol",
      "title": "AVSampleBufferRenderSynchronizer",
      "type": "topic",
      "url": "/documentation/AVFoundation/AVSampleBufferRenderSynchronizer"
    },
    "doc://com.apple.documentation/documentation/technologies": {
      "abstract": [
        {
          "text": "",
          "type": "text"
        }
      ],
      "identifier": "doc://com.apple.documentation/documentation/technologies",
      "kind": "technologies",
      "role": "overview",
      "title": "Technologies",
      "type": "topic",
      "url": "/documentation/technologies"
    }
  },
  "sampleCodeDownload": {
    "action": {
      "identifier": "68f077a79b68/PlayingCustomAudioWithYourOwnPlayer.zip",
      "isActive": true,
      "overridingTitle": "Download",
      "type": "reference"
    },
    "kind": "sampleDownload"
  },
  "schemaVersion": {
    "major": 0,
    "minor": 3,
    "patch": 0
  },
  "sections": [],
  "seeAlsoSections": [
    {
      "anchor": "Playback",
      "generated": true,
      "identifiers": [
        "doc://com.apple.avfaudio/documentation/AVFAudio/using-voice-processing",
        "doc://com.apple.avfaudio/documentation/AVFAudio/AVAudioPlayerNode"
      ],
      "title": "Playback"
    }
  ],
  "variantOverrides": [
    {
      "patch": [
        {
          "op": "replace",
          "path": "/identifier/interfaceLanguage",
          "value": "occ"
        },
        {
          "op": "add",
          "path": "/topicSections",
          "value": null
        },
        {
          "op": "replace",
          "path": "/seeAlsoSections",
          "value": [
            {
              "anchor": "Playback",
              "generated": true,
              "identifiers": [
                "doc://com.apple.avfaudio/documentation/AVFAudio/using-voice-processing",
                "doc://com.apple.avfaudio/documentation/AVFAudio/AVAudioPlayerNode"
              ],
              "title": "Playback"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.documentation~1documentation~1AVFoundation~1AVSampleBufferAudioRenderer/fragments",
          "value": [
            {
              "kind": "keyword",
              "text": "@interface"
            },
            {
              "kind": "text",
              "text": " "
            },
            {
              "kind": "identifier",
              "text": "AVSampleBufferAudioRenderer"
            },
            {
              "kind": "text",
              "text": " : "
            },
            {
              "kind": "typeIdentifier",
              "preciseIdentifier": "c:objc(cs)NSObject",
              "text": "NSObject"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.avfaudio~1documentation~1AVFAudio~1AVAudioPlayerNode/title",
          "value": "AVAudioPlayerNode"
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.avfaudio~1documentation~1AVFAudio~1AVAudioPlayerNode/fragments",
          "value": [
            {
              "kind": "identifier",
              "text": "AVAudioPlayerNode"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.avfaudio~1documentation~1AVFAudio~1AVAudioPlayerNode/navigatorTitle",
          "value": [
            {
              "kind": "identifier",
              "text": "AVAudioPlayerNode"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.avfaudio~1documentation~1AVFAudio~1AVAudioSession/title",
          "value": "AVAudioSession"
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.avfaudio~1documentation~1AVFAudio~1AVAudioSession/fragments",
          "value": [
            {
              "kind": "identifier",
              "text": "AVAudioSession"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.avfaudio~1documentation~1AVFAudio~1AVAudioSession/navigatorTitle",
          "value": [
            {
              "kind": "identifier",
              "text": "AVAudioSession"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.avfaudio~1documentation~1AVFAudio~1AVAudioSession~1RouteSharingPolicy-swift.enum~1longFormAudio/title",
          "value": "AVAudioSessionRouteSharingPolicyLongFormAudio"
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.avfaudio~1documentation~1AVFAudio~1AVAudioSession~1RouteSharingPolicy-swift.enum~1longFormAudio/fragments",
          "value": [
            {
              "kind": "identifier",
              "text": "AVAudioSessionRouteSharingPolicyLongFormAudio"
            }
          ]
        },
        {
          "op": "replace",
          "path": "/references/doc:~1~1com.apple.documentation~1documentation~1AVFoundation~1AVSampleBufferRenderSynchronizer/fragments",
          "value": [
            {
              "kind": "keyword",
              "text": "@interface"
            },
            {
              "kind": "text",
              "text": " "
            },
            {
              "kind": "identifier",
              "text": "AVSampleBufferRenderSynchronizer"
            },
            {
              "kind": "text",
              "text": " : "
            },
            {
              "kind": "typeIdentifier",
              "preciseIdentifier": "c:objc(cs)NSObject",
              "text": "NSObject"
            }
          ]
        }
      ],
      "traits": [
        {
          "interfaceLanguage": "occ"
        }
      ]
    }
  ],
  "variants": [
    {
      "paths": [
        "/documentation/avfaudio/playing-custom-audio-with-your-own-player"
      ],
      "traits": [
        {
          "interfaceLanguage": "swift"
        }
      ]
    },
    {
      "paths": [
        "/documentation/avfaudio/playing-custom-audio-with-your-own-player"
      ],
      "traits": [
        {
          "interfaceLanguage": "occ"
        }
      ]
    }
  ]
}
